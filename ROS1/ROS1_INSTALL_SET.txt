


## https://github.com/IntelRealSense/realsense-ros/pull/1981
## Allow T265 to save/load localization maps #1981


## https://github.com/IntelRealSense/realsense-ros/issues/1155
## T265 Kidnapped robot ? where is the database ? #1155


------------------------------------------------------------------------------------------
## SLAM with D435i with ROS1 kinetic
------------------------------------------------------------------------------------------
## https://github.com/IntelRealSense/realsense-ros/wiki/SLAM-with-D435i

##  Jan 23, 2019 · 6 revisions



SLAM with RealSense™ D435i camera on ROS:

The RealSense™ D435i is equipped with a built in IMU. Combined with some powerful open source tools, it's possible to achieve the tasks of mapping and localization.

There are 4 main nodes to the process:

realsense2_camera
imu_filter_madgwick
rtabmap_ros
robot_localization

## Installation:

The first thing to do is to install the components:
realsense2_camera: Follow the installation guide in: https://github.com/intel-ros/realsense.
imu_filter_madgwick: sudo apt-get install ros-kinetic-imu-filter-madgwick
rtabmap_ros: sudo apt-get install ros-kinetic-rtabmap-ros
robot_localization: sudo apt-get install ros-kinetic-robot-localization

## Running:
Hold the camera steady with a clear view and run the following command:

roslaunch realsense2_camera opensource_tracking.launch
Wait a little for the system to fix itself.

## Personalize RViz:
The pointcloud and a bunch of arrows, or axes marks, will appear on screen. These axes represent all the different coordinate systems involved. For clarity you could remove most of them.

From the Displays Panel:
TF -> Frames, and then leave out as marked only map and camera_link. The first represents the world coordinate system and the second, the camera.

You may want to watch the on-line video as well:
From the Displays panel:
Image->Image Topic: rewrite to /camera/color/image_raw


...


Start moving around and watch the “camera_link” axes mark moving accordingly, in regards to the “map” axes.
Notice:
The built-in IMU can only keep track for a very short time. Moving or turning too quickly will break the sequence of successful point cloud matches and will result in the system losing track. It could happen that the system will recover immediately if stopped moving but if not, the longer the time passed since the break, the farther away it will drift from the correct position. The odds for recovery get very slim, very quickly. The parameters set in the launch file are most likely not ideal but this is a good starting point for calibrating.

For saving a rosbag file you may use the following command:

rosbag record -O my_bagfile_1.bag /camera/aligned_depth_to_color/camera_info  camera/aligned_depth_to_color/image_raw /camera/color/camera_info /camera/color/image_raw /camera/imu /camera/imu_info /tf_static
To replay a saved rosbag file:

roscore >/dev/null 2>&1 &
rosparam set use_sim_time true
rosbag play my_bagfile_1.bag --clock
roslaunch realsense2_camera opensource_tracking.launch offline:=true
The process looks like that:
...

and the resulting point cloud:
...

While the system is up, you can create a 2D map using:

rosrun map_server map_saver map:=/rtabmap/proj_map –f my_map_1
and a resulting map would look something like that:

...


You can save the point cloud using:

@@@@@
rosrun pcl_ros pointcloud_to_pcd input:=/rtabmap/cloud_map <<<<<<<<<-----------
@@@@@

Notice: The app will keep on saving pointsclouds.
Use ctrl-C to stop it after it reports saving the 1st file.
The app prints to screen the file names it saves. For example: 1543906154413083.pcd

You can watch the saved point cloud later using:

pcl_viewer 1543906154413083.pcd
[Install using: sudo apt-get install pcl-tools]














# https://github.com/introlab/rtabmap_ros/tree/master/docker
# https://github.com/introlab/rtabmap_ros/issues/943

# RTABMap Ros Noetic Installation on NVIDIA Jetson  #943


Based on https://github.com/introlab/rtabmap_ros/tree/master/docker

docker pull introlab3it/rtabmap_ros:noetic-latest

Launch rtabmap node:

mkdir ~/.ros # make sure ~/.ros exists
docker run -it --rm \
   --user $UID \
   -e ROS_HOME=/tmp/.ros \
   --network host \
   -v ~/.ros:/tmp/.ros \
    introlab3it/rtabmap_ros:noetic-latest \
    roslaunch rtabmap_launch rtabmap.launch rtabmap_viz:=false database_path:=/tmp/.ros/rtabmap.db rtabmap_args:="--delete_db_on_start"

For visualization (ideally do it on remote computer), use rviz, or rtabmap_viz can be launched like this:

XAUTH=/tmp/.docker.xauth
touch $XAUTH
xauth nlist $DISPLAY | sed -e 's/^..../ffff/' | xauth -f $XAUTH nmerge -

docker run -it --rm \
   --privileged \
   -e DISPLAY=$DISPLAY \
   -e QT_X11_NO_MITSHM=1 \
   -e NVIDIA_VISIBLE_DEVICES=all \
   -e NVIDIA_DRIVER_CAPABILITIES=all \
   -e XAUTHORITY=$XAUTH \
   --runtime=nvidia \
   --network host \
   -v $XAUTH:$XAUTH \
   -v /tmp/.X11-unix:/tmp/.X11-unix \
   -e LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/aarch64-linux-gnu:/usr/lib/aarch64-linux-gnu/tegra \
    introlab3it/rtabmap_ros:noetic-latest \
    /bin/bash -c "export ROS_NAMESPACE=rtabmap && rosrun rtabmap_viz rtabmap_viz"





............

## INSTALL g2o
https://github.com/rubengooj/pl-slam/issues/1

git clone https://github.com/RainerKuemmerle/g2o.git
cd g2o
mkdir build
cd build
cmake ..
make
sudo make install

### sudo cp /home/jetson/ros2_ws/src/slam_toolbox/CMake/FindG2O.cmake /usr/share/cmake-3.16/Modules/

sudo ldconfig


## BUILD OK

## INSTALL  If using SocketViewer)

sudo apt install -y autogen autoconf libtool
cd /tmp
git clone https://github.com/shinsumicco/socket.io-client-cpp.git
cd socket.io-client-cpp
git submodule init
git submodule update
mkdir build && cd build
cmake \
    -DCMAKE_BUILD_TYPE=Release \
    -DCMAKE_INSTALL_PREFIX=/usr/local \
    -DBUILD_UNIT_TESTS=OFF \
    ..
make -j4
sudo make install
sudo apt install -y libprotobuf-dev protobuf-compiler
wget -q https://github.com/google/protobuf/archive/v3.6.1.tar.gz
tar xf v3.6.1.tar.gz
cd protobuf-3.6.1
./autogen.sh
./configure \
    --prefix=/usr/local \
    --enable-static=no
make -j4
sudo make install

## BUILD OK






------------------------------------------------------------------------------------------
# stella_vslam_ros
------------------------------------------------------------------------------------------

# https://stella-cv.readthedocs.io/en/latest/ros_package.html#installation

Tested for Ubuntu 20.04.
ROS : noetic.




rosdep update
sudo apt update
mkdir -p ~/lib
cd ~/lib
git clone --recursive --depth 1 https://github.com/stella-cv/stella_vslam.git
rosdep install -y -i --from-paths ~/lib
cd ~/lib/stella_vslam
mkdir -p ~/lib/stella_vslam/build
cd ~/lib/stella_vslam/build
source /opt/ros/${ROS_DISTRO}/setup.bash
cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo ..
make -j
sudo make install

# When building with support for IridescenceViewer
...


# When building with support for PangolinViewer
cd ~/lib
git clone --recursive https://github.com/stella-cv/pangolin_viewer.git
mkdir -p pangolin_viewer/build
cd pangolin_viewer/build
cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo ..
make -j
sudo make install

# When building with support for SocketViewer
...

mkdir -p ~/catkin_ws/src
cd ~/catkin_ws/src
git clone --recursive -b ros --depth 1 https://github.com/stella-cv/stella_vslam_ros.git
cd ~/catkin_ws/
rosdep install -y -i --from-paths ~/catkin_ws/src --skip-keys=stella_vslam
catkin_make -j


# BUILD OK


# download an ORB vocabulary from GitHub
    cd ${TOPDIR}/vocab
    curl -sL "https://github.com/stella-cv/FBoW_orb_vocab/raw/main/orb_vocab.fbow" -o orb_vocab.fbow


-----------------------------
(if you plan on using SocketViewer)
-----------------------------

Download, build and install socket.io-client-cpp from source.
cd /tmp
git clone https://github.com/shinsumicco/socket.io-client-cpp.git
cd socket.io-client-cpp
git submodule init
git submodule update
mkdir build && cd build
cmake \
    -DCMAKE_BUILD_TYPE=Release \
    -DCMAKE_INSTALL_PREFIX=/usr/local \
    -DBUILD_UNIT_TESTS=OFF \
    ..
make -j4
sudo make install


Install Protobuf.
If you use Ubuntu 20.04 (or later) or macOS, Protobuf 3.x can be installed via apt or brew.

# for Ubuntu
apt install -y libprotobuf-dev protobuf-compiler
# for macOS
brew install protobuf


-----------------------------
# When building with support for SocketViewer
-----------------------------

cd ~/lib
git clone --recursive https://github.com/stella-cv/socket_publisher.git
mkdir -p socket_publisher/build
cd socket_publisher/build
cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo ..
make -j
sudo make install


## BUILD OK!

-----------------------------
Server Setup for SocketViewer
-----------------------------
If you plan on using SocketViewer, please setup the environment for the server with npm.

cd ~/cart_ws/src
git clone --recursive https://github.com/stella-cv/socket_viewer.git
cd socket_viewer
ls

Dockerfile  app.js  package.json  public  views

npm install
added 88 packages from 60 contributors and audited 204 packages in 2.105s
found 0 vulnerabilities

ls
Dockerfile  app.js  node_modules  package-lock.json  package.json  public  views


Then, launch the server with node app.js.

git clone --recursive https://github.com/stella-cv/socket_viewer.git
cd socket_viewer

ls
Dockerfile  app.js  node_modules  package-lock.json  package.json  public  views

node app.js
WebSocket: listening on *:3000
HTTP server: listening on *:3001
After launching, please access to http://localhost:3001/ to check whether the server is correctly launched.


## RUN, BUT WHITE SCREEN, NO GRIG...


-----------------------------


# RUN:

roscore 

source /home/jetson/cart_ws/devel/setup.bash

source /opt/ros/noetic/setup.bash
## rosrun usb_cam usb_cam_node 
##NOT WORK...

# RUN usb_cam:
roslaunch usb_cam usb_cam-test.launch
rosrun image_transport republish raw in:=/usb_cam/image_raw raw out:=/camera/image_raw




# RUN t265:
realsense-viewer

roslaunch realsense2_camera rs_t265.launch

rosrun image_transport republish raw in:=/camera/fisheye1/image_raw raw out:=/camera/image_raw

--ros-args -r /camera/image_raw:=/cam0/image_raw


rqt

rosrun rqt_tf_tree rqt_tf_tree

rosrun stella_vslam_ros run_slam \
-v /home/jetson/lib/stella_vslam/vocab/orb_vocab.fbow \
-c /home/jetson/lib/stella_vslam/example/tum_vi/RealSense_T265.yaml \
--map-db-out /home/jetson/cart_ws/src/omni/map/stella_vslam/map.msg 


rosrun stella_vslam_ros run_slam -v /home/jetson/lib/stella_vslam/vocab/orb_vocab.fbow -c /home/jetson/lib/stella_vslam/example/tum_vi/RealSense_T265.yaml --viewer socket_publisher


--map-db-out /home/jetson/cart_ws/src/omni/map/stella_vslam/map.msg 



--viewer socket_publisher
--viewer pangolin_viewer

rosrun stella_vslam_ros run_slam \
-v /home/jetson/lib/stella_vslam/vocab/orb_vocab.fbow \
-c /home/jetson/lib/stella_vslam/example/tum_vi/RealSense_T265.yaml \
--map-db-out /home/jetson/cart_ws/src/omni/map/stella_vslam/map.msg --viewer none


source /home/jetson/cart_ws/devel/setup.bash

roslaunch omni omni_stella_vslam.launch


root@hostname:/ros2_ws# ros2 run tf2_ros static_transform_publisher 0 0 0 0 0 0 odom base_link

root@hostname:/ros2_ws# ros2 run tf2_ros static_transform_publisher 0 0 0 0 0 0 base_link cam0






## WORK???



rostopic echo /camera/fisheye1/camera_info


header: 
  seq: 50163
  stamp: 
    secs: 1739103647
    nsecs: 702287674
  frame_id: "camera_fisheye1_optical_frame"
height: 800
width: 848
distortion_model: "equidistant"
D: [-0.00019197289657313377, 0.0347595289349556, -0.034760478883981705, 0.005636353045701981]
K: [284.7441101074219, 0.0, 420.2145080566406, 0.0, 285.9248046875, 393.724609375, 0.0, 0.0, 1.0]
R: [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
P: [284.7441101074219, 0.0, 420.2145080566406, 0.0, 0.0, 285.9248046875, 393.724609375, 0.0, 0.0, 0.0, 1.0, 0.0]
binning_x: 0
binning_y: 0
roi: 
  x_offset: 0
  y_offset: 0
  height: 0
  width: 0
  do_rectify: False
---













# Republish the ROS topic to /camera/image_raw.

rosrun image_transport republish raw in:=/usb_cam/image_raw raw out:=/camera/image_raw

rosrun image_view image_view image:=/usb_cam/image_raw






------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
# ROS1 SYSTEM
------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------

ROS1 - noetic, foxy, galactic, humble, iron
ROS2 - Jazzy (latest) Iron (EOL) Humble Galactic (EOL) Foxy (EOL) Eloquent (EOL) Dashing (EOL) Crystal (EOL)

export ROS_VER=noetic
export ROS_DISTRO=noetic

# FIND ROS POCKET # This command will return the package path if it's installed.
rospack find usb_cam
/opt/ros/noetic/share/usb_cam

roscd usb_cam

rospack find usb_cam

realsense-viewer


Check topics / services/ parameters (open a new terminal)
rostopic list
rosservice list
rosparam list

Can you please share your TF tree:
$ rosrun rqt_tf_tree rqt_tf_tree



------------------------------------------------------------------------------------------
# rqt graph – Visualize and Debug Your ROS Graph
------------------------------------------------------------------------------------------

## https://roboticsbackend.com/rqt-graph-visualize-and-debug-your-ros-graph/

rosrun rqt_graph rqt_graph


## CHECK ERRORS:
roswtf


## CHECK TF_TREE:
rosrun rqt_tf_tree rqt_tf_tree





------------------------------------------------------------------------------------------
## ROS1 How to Create a New Package in ROS Noetic
------------------------------------------------------------------------------------------


## CREATE PACKAGE:

catkin_create_pkg <package_name> [depend1] [depend2] [depend3]

cd ~/ros_ws/src

catkin_create_pkg omni rospy std_msgs roscpp

cd ..

## BUILD:

catkin build


source /home/jetson/ros_ws/devel/setup.bash


## RUN:

$ chmod u+x '/home/jetson/ros_ws/src/omni/src/listener.py'
$ chmod u+x '/home/jetson/ros_ws/src/omni/src/talker.py' 

T1:
rosrun omni listener.py

T2:
rosrun omni talker.py




$ time rosbag info   '/home/jetson/Downloads/cartographer_paper_revo_lds.bag'

path:         /home/jetson/Downloads/cartographer_paper_revo_lds.bag
version:      2.0
duration:     16:30s (990s)
start:        Jul 17 2015 12:06:59.42 (1437124019.42)
end:          Jul 17 2015 12:23:29.64 (1437125009.64)
size:         3.0 MB
messages:     4952
compression:  bz2 [19/19 chunks; 20.97%]
uncompressed: 14.2 MB @ 14.6 KB/s
compressed:    3.0 MB @  3.1 KB/s (20.97%)

types:        sensor_msgs/LaserScan [90c7ef2dc6895d81024acba2ac42f369]

topics:       horizontal_laser_2d   4952 msgs    : sensor_msgs/LaserScan

real	0m0.627s
user	0m0.621s
sys	0m0.059s






https://github.com/ROBOTIS-GIT/turtlebot3/issues/307


https://github.com/flg-vs/OpenCR/blob/7404d3b905f6fad9b289b8b85112ffdaecd22337/arduino/opencr_arduino/opencr/libraries/turtlebot3/examples/turtlebot3_friends/turtlebot3_mecanum_core/turtlebot3_mecanum_core.ino



https://github.com/ROBOTIS-GIT/turtlebot3/issues/710



------------------------------------------------------------------------------------------
## ROS1 Cartographer ROS
------------------------------------------------------------------------------------------
## https://google-cartographer-ros.readthedocs.io/en/latest/compilation.html#building-installation


# INSTALL:

sudo apt-get update

sudo apt-get install -y python3-wstool python3-rosdep ninja-build stow


mkdir cart_ws
cd cart_ws

wstool init src
wstool merge -t src https://raw.githubusercontent.com/cartographer-project/cartographer_ros/master/cartographer_ros.rosinstall
wstool update -t src


sudo rosdep init
rosdep update
#rosdep install --from-paths src --ignore-src --rosdistro=${ROS_DISTRO} -y
rosdep install --from-paths src --ignore-src --rosdistro=noetic -y

# ERROR:
cartographer: [libabsl-dev] defined as "not available" for OS version [focal]

# FIX:
# https://github.com/cartographer-project/cartographer_ros/issues/1726

Well, as it turns out that this dependency was added to the cartographer package roughly a week ago (as can be seen here https://github.com/cartographer-project/cartographer/pull/1875); 
removing the respective line from the 
package.xml and installing abseil via the command from the guide (ie. src/cartographer/scripts/install_abseil.sh) seems to have fixed the issue, atleast in so far as it allowed me to build and install cartographer ros.

I'm not 100% sure if it fixed the issue without creating new ones though.

PS: I accidentally closed the topic instead of simply posting a comment, so I've reopened it in case anyone else wants to contribute.

## BUILD START...

Processing triggers for man-db (2.9.1-1) ...
#All required rosdeps installed successfully

## BUILD OK


src/cartographer/scripts/install_abseil.sh


Due to conflicting versions you might need to uninstall the ROS abseil-cpp using
#sudo apt-get remove ros-${ROS_DISTRO}-abseil-cpp
sudo apt-get remove ros-noetic-abseil-cpp
## error
E: Unable to locate package ros-noetic-abseil-cpp
...


catkin_make_isolated --install --use-ninja
## ERROR 
Extension error: Could not import extension sphinx.builders.latex (exception: cannot import name 'contextfunction' from 'jinja2'


# FIX:
pip install jinja2==3.0.3


## BUILD OK

<== Finished processing package [4 of 4]: 'cartographer_rviz'



## RUN:

Download and launch an example bag captured from a low-cost Revo Laser Distance Sensor from Neato Robotics vacuum cleaners:

wget -P ~/Downloads https://storage.googleapis.com/cartographer-public-data/bags/revo_lds/cartographer_paper_revo_lds.bag

source /home/jetson/cart_ws/devel_isolated/setup.bash

roslaunch cartographer_ros demo_revo_lds.launch bag_filename:=${HOME}/Downloads/cartographer_paper_revo_lds.bag


roslaunch hls_lfcd_lds_driver view_hlds_laser.launch
roslaunch hls_lfcd_lds_driver hlds_laser.launch


## ERROR:
sensor_bridge.cc:240] Ignored subdivision of a LaserScan message from sensor scan because previous subdivision time 621355968001996644 is not before current subdivision time 621355968001989966

## FIX:

# https://github.com/cartographer-project/cartographer_ros/issues/1326


The problem is: time_increment: 0.0. This needs to be set to the correct value, otherwise Cartographer's scan unskewing doesn't work correctly. See also http://docs.ros.org/en/api/sensor_msgs/html/msg/LaserScan.html

An approximation could be calculated with time_increment = scan_time / ranges.size().


rostopic echo /horizontal_laser_2d
--
header: 
  seq: 684
  stamp: 
    secs: 1738699952
    nsecs: 726814311
  frame_id: "laser"
angle_min: 0.0
angle_max: 6.2657318115234375
angle_increment: 0.01745329238474369

time_increment: 0.0005592841189354658 <<<<<<<<<< = 0.20134228467941284 / 360

scan_time: 0.20134228467941284
range_min: 0.11999999731779099
range_max: 3.5
ranges: [0.0, 0.0, 4.131999969482422, 4.053999900817871, 0.0, 4.071000099182129, 0.0, 0.0, 0.0, 2.878999948501587, 2.864000082015991, 2.8550000190734863, 2.8489999771118164, 2.8440001010894775, 2.83299994


<<<<<<<<<<<<<<<<<<<<< MY LIDAR >>>>>>>>>>>>>>>>>>>>>>>>>
$ roslaunch hls_lfcd_lds_driver hlds_laser.launch
$ rostopic echo /scan
--
header: 
  seq: 1474
  stamp: 
    secs: 1738703177
    nsecs: 407700241
  frame_id: "laser"
angle_min: 0.0
angle_max: 6.2657318115234375
angle_increment: 0.01745329238474369
time_increment: 0.0005592841189354658 <<<<<<<<<< = 0.20134228467941284 / 360
scan_time: 0.20134228467941284
range_min: 0.11999999731779099
range_max: 3.5
ranges: [2.822999954223633, 2.7769999504089355, 2.7780001163482666, 2.75600004196167, 2.7869999408721924, 2.746999979019165, 2.7309999465942383, 2


## MAKE ROBAG
rosbag record -O mylaserdata /scan /tf
rosbag record -O mylaserdata /scan /tf /use_sim_time:=true


<<<<<<<<<<<<<<<<<<<<< MY LIDAR >>>>>>>>>>>>>>>>>>>>>>>>>


rosbag play --clock  '/home/jetson/Downloads/cartographer_paper_revo_lds.bag'

rostopic list

/clock <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/horizontal_laser_2d
/rosout
/rosout_agg

rostopic echo clock
---
clock: 
  secs: 1437124034
  nsecs: 767928854
---
clock: 
  secs: 1437124034
  nsecs: 777990765
---
clock: 
  secs: 1437124034
  nsecs: 788049956



rostopic echo /horizontal_laser_2d
--
header: 
  seq: 0
  stamp: 
    secs: 1437124034
    nsecs: 618380000
  frame_id: "horizontal_laser_link"
angle_min: 0.0
angle_max: 6.2831854820251465
angle_increment: 0.01745329238474369
time_increment: 0.0
scan_time: 0.0
range_min: 9.999999974752427e-07
range_max: 5.0
ranges: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0140000581741333, 1.003999948501587, 0.9980000257492065, 0.9129999876022339, 0.9279999732971191, 0.0, 0.0, 0.0, 0.0, 0.7749999761581421, 0.7699999809265137, 
...]

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
## https://github.com/cartographer-project/cartographer_ros/pull/1284
# Add function to set the initial pose from RVIZ for pure localization mode #1284

As mentioned in the closed PR #637, manually "tuning" of the robot initial pose from GUI (e.g. Rviz) is a very important function for home service robot, which is supported by the ROS default localization method AMCL.

I hope this PR can facilitate the development of this RFC.

To test my commit, please try following commands

First follow the tutorial about pure localization to download rosbags and build the map (.pbstream).
Then run following command instead:
$ roslaunch cartographer_ros demo_backpack_2d_localization.launch \
   load_state_filename:=${HOME}/Downloads/b3-2016-04-05-13-54-42.bag.pbstream \
   rosbag:=false



<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# https://github.com/cartographer-project/cartographer_ros/issues/1652
# Python script to set initial pose for pure-localization via rviz  #1652


Maybe you want to add this to the Cartographer 'scripts' folder. It's a simple Python script that allows to run 'start_trajectory' service calls simply with the pose clicked via ROS rviz '2D Pose Estimate' button. I think it's a good start for users and it does not touch any Cartographer code.

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

$ rosrun tf2_ros static_transform_publisher 0 0 0 0 0 0 1 odom map
[ INFO] [1738702423.244798540]: Spinning until killed publishing odom to map



 rostopic echo /tf_static
transforms: 
  - 
    header: 
      seq: 0
      stamp: 
        secs: 1738702423
        nsecs: 244634950
      frame_id: "odom"
    child_frame_id: "map"
    transform: 
      translation: 
        x: 0.0
        y: 0.0
        z: 0.0
      rotation: 
        x: 0.0
        y: 0.0
        z: 0.0
        w: 1.0




## https://github.com/cartographer-project/cartographer/issues/1569
## Error using Odometry data in 2d Cartographer #1569

So,
I recorded my bag once again including the /clock as well this time. I ran the same configuration by setting use_odometry = true and odometry_sampling_ratio = 0.1 and it is working now.
But I'm getting warning messages as you can see in the image.


## Validate your bag
## https://google-cartographer-ros.readthedocs.io/en/latest/your_bag.html?highlight=validate#validate-your-bag




}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}
##https://google-cartographer-ros.readthedocs.io/en/latest/your_bag.html?highlight=validate#create-launch-files-for-your-slam-scenarios

## Create .launch files for your SLAM scenarios


Start by copying one of the provided example:

cp install_isolated/share/cartographer_ros/launch/backpack_3d.launch install_isolated/share/cartographer_ros/launch/my_robot.launch


cp install_isolated/share/cartographer_ros/launch/backpack_2d.launch install_isolated/share/cartographer_ros/launch/my_robot.launch



my_robot.launch is meant to be used on the robot to execute SLAM online (in real time) with real sensors data.


## RUN:

source /home/jetson/cart_ws/devel_isolated/setup.bash

video-viewer --input-width=640 --input-height=480 csi://0 > /dev/null

roslaunch hls_lfcd_lds_driver hlds_laser.launch
rostopic echo /scan



# roslaunch demo_revo_lds.launch

# roslaunch cartographer_ros demo_revo_lds.launch

roslaunch cartographer_ros my_robot.launch





[ERROR] [1738754427.845711042]: 
Client [/cartographer_node] wants topic 
/scan to have datatype/md5sum [sensor_msgs/MultiEchoLaserScan/6fefb0c6da89d7c8abe4b339f5c2f8fb], 
but our version has           [sensor_msgs/LaserScan/90c7ef2dc6895d81024acba2ac42f369]. Dropping connection.



{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{



||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
## https://medium.com/robotics-weekends/2d-mapping-using-google-cartographer-and-rplidar-with-raspberry-pi-a94ce11e44c5
## 2D Mapping using Google Cartographer and RPLidar with Raspberry Pi
## https://github.com/Andrew-rw/gbot_core.git
# gbot_core
# Set of ROS parameter files and scripts to launch Cartographer SLAM
# Refer to Youtube video and Medium story for the details:

## INSTALL:
git clone https://github.com/Andrew-rw/gbot_core.git


## CHANGE: in "gbot.launch"
# THIS:
<!-- Start RPLIDAR sensor node which provides LaserScan data  -->
    <!--node name="rplidarNode" pkg="rplidar_ros" type="rplidarNode" output="screen">
        <param name="serial_port" type="string" value="/dev/ttyUSB0"/>
        <param name="serial_baudrate" type="int" value="115200"/>
        <param name="frame_id" type="string" value="laser"/>
        <param name="inverted" type="bool" value="false"/>
        <param name="angle_compensate" type="bool" value="true"/>
    </node-->
    
# TO THIS:
  <node pkg="hls_lfcd_lds_driver" type="hlds_laser_publisher" name="hlds_laser_publisher" output="screen">
    <param name="port" value="/dev/ttyUSB0"/>
    <param name="frame_id" value="laser"/>
  </node>

# AND PATH NAMES: $(find gbot_core) to abs path "/home/jetson/cart_ws/src/gbot_core/"


###### RUN:

~/cart_ws/devel$ roslaunch /home/jetson/cart_ws/src/gbot_core/launch/gbot.launch

## WORK!!!

rosrun map_server map_saver -f /tmp/my_map

This creates two files. The YAML file describes the map metadata, and names the image file. The image file encodes the occupancy data. An example image of the occupancy grid map is shown below:



||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||

## REAL WORLD EXAMPLES::
## https://ouster.com/insights/blog/building-maps-using-google-cartographer-and-the-os1-lidar-sensor
## Building Maps Using Google Cartographer and the OS1 Lidar Sensor


# https://answers.ros.org/question/267787/
# Tuning Google Cartographer to work with a Velodyne HDL-32


## https://github.com/cartographer-project/cartographer/issues/1569
## Error using Odometry data in 2d Cartographer #1569

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

## RUN:

source /home/jetson/cart_ws/devel_isolated/setup.bash

video-viewer --input-width=640 --input-height=480 csi://0 > /dev/null

roscore

# roslaunch hls_lfcd_lds_driver hlds_laser.launch
# rostopic echo /scan


# roslaunch cartographer_ros demo_revo_lds.launch
# roslaunch cartographer_ros my_robot.launch
# roslaunch cartographer_ros omni.launch
roslaunch omni omni.launch

#### 

source /home/jetson/cart_ws/devel/setup.bash

video-viewer --input-width=640 --input-height=480 csi://0 > /dev/null

roscore


roscd omni
roslaunch omni omni.launch

rosrun map_server map_saver -f


---


------------------------------------------------------------------------------------------
## Building a Map Using LiDAR with ROS Melodic on Jetson Nano and Hector SLAM
------------------------------------------------------------------------------------------
## https://www.hackster.io/shahizat/building-a-map-using-lidar-with-ros-melodic-on-jetson-nano-2f92dd


git clone https://github.com/tu-darmstadt-ros-pkg/hector_slam.git
...

## WORK...

## RUN:

source /home/jetson/cart_ws/devel/setup.bash

roslaunch hector_slam_launch tutorial.launch



## RELOCALIZATION
# https://github.com/tu-darmstadt-ros-pkg/hector_slam/pull/45


I got localization and continuous mapping done with this pull request (Noetic).
(1) Merge this PR with the current up-to-date branch, build, and source.
(2) In your launch files add map_server node, something like

<arg name="map_file" default="/path/to/map.yaml"/> 
<node name="map_server" pkg="map_server" type="map_server" args="$(arg map_file)" />
If you need to get a map, install map-server

sudo apt-get install ros-noetic-map-server
After this run your hector-mapping code to build your map, once your mapping is finished run

rosrun map_server map_saver --occ 90 --free 10 -f map
to get your map.
(3) Add the "use_static_map" to the hector_mapping rosnode, mine ended up looking as follows

<node pkg="hector_mapping" type="hector_mapping" name="hector_mapping" output="screen">
  <param name="map_resolution" value="0.050" />
  <param name="map_size" value="2048" />
  <param name="update_factor_free" value="Put a Float Value Here from 0 to 1, see docs and below info"/>
  <param name="update_factor_occupied" value="Put a Float Value Here from 0 to 1, see docs and below info"/>
  <param name="scan_subscriber_queue_size" value="5" />
  <param name="scan_topic" value="/scan" />
  <param name="pub_map_odom_transform" value="true"/>
  <param name="use_static_map" value="true"/>
  <param name="map_frame" value="map" />
  <param name="base_frame" value="base_link" />
  <param name="odom_frame" value="odom" />
</node>
(4) If you want to do continuous mapping, the default values are 0.4 and 0.9 for the free and occupied values respectively. However, I wanted faster updates to my map and used the values 0.1 and 0.99999. If you want to do localization and do not want to update the map set the values to 0.5 and 0.5. See the documentation and code for Hector-SLAM to get a better idea for the motivation for these values.
(5) Use AMCL to get an estimate of the starting location, and publish this pose to /initialpose. I've found that AMCL gives a good enough starting estimate and after that hector-slam converges nicely. Alternatively, you can use RVIZ's "2D Pose Estimate" button to have an interactive method for giving the /initialpose.

Hope this helps.




---

Convert the Map into png Format
To convert the map to a png image file, you can use imagemagick.

sudo apt-get install imagemagick
convert my_map.pgm my_map.png
Edit the Map
To edit the map, I recommend using a program like GIMP.

Install gimp.

sudo apt-get update
sudo apt-get install gimp
Run gimp.

gimp




## https://linuxize.com/post/how-to-setup-ssh-tunneling/
ssh -L 5901:127.0.0.1:5901 -N -f -l jetson@192.168.2.34
## NOT WORK...





------------------------------------------------------------------------------------------
## ROS1 360 Laser Distance Sensor LDS-01 is a 2D laser scanner 
------------------------------------------------------------------------------------------
## error: For frame [laser]: Frame [laser] does not exist
The line extraction node is trying to publish the markers in the laser frame, which doesn't exist because you probably haven't provided a transform. 
You should set the "frame_id" parameter (see package parameter definitions here) to a frame which is defined in your tf_tree.

## https://github.com/kam3k/laser_line_extraction#parameters
## Laser Line Extraction


## ADD THIS TO launch file
# https://answers.ros.org/question/215224/
<launch>

  <node pkg="slam_toolbox" type="localization_slam_toolbox_node" name="slam_toolbox" output="screen">
    <rosparam command="load" file="$(find slam_toolbox)/config/mapper_params_localization.yaml" />
    <param name="frame_id" value="laser"/> #<<<<<<<<<
  </node>
  <node pkg="tf" type="static_transform_publisher" name="base_to_laser_broadcaster"  args="0 0 0 0 0 0 /base_link /laser 100" />  #<<<<<<<<<

</launch>




## INSTALL:

$ sudo apt-get install ros-noetic-hls-lfcd-lds-driver

Set Permission for LDS-01
$ sudo chmod a+rw /dev/ttyUSB0

# LOCATE: /opt/ros/noetic/share/hls_lfcd_lds_driver/

## RUN:

Run hlds_laser_publisher Node
roslaunch hls_lfcd_lds_driver hlds_laser.launch

Run hlds_laser_publisher Node with RViz
roslaunch hls_lfcd_lds_driver view_hlds_laser.launch
## WORK!!

roslaunch hls_lfcd_lds_driver view_hlds_laser_segment_publisher.launch






------------------------------------------------------------------------------------------
## ROS1 How to build a Map Using Logged Data -- hector_slam
------------------------------------------------------------------------------------------

# https://wiki.ros.org/hector_slam/Tutorials/MappingUsingLoggedData

## INSTALL:
sudo apt-get install ros-noetic-hector-slam

# Get a bagfile

wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/tu-darmstadt-ros-pkg/Team_Hector_MappingBox_RoboCup_2011_Rescue_Arena.bag

source /home/jetson/ast_ws/revel/setup.bash

roslaunch hector_slam_launch tutorial.launch

@@@@@@## RUN, but map is very bad...





## RUN slam:
# according to this - https://answers.ros.org/question/215224/


roslaunch hector_mapping slam.launch






------------------------------------------------------------------------------------------
## ROS1 ROBOTICS EVALUATION TOOLKITS in warehouse environment
------------------------------------------------------------------------------------------
# A simulation of basic robot localization (SLAM) and navigation in warehouse environment

# https://github.com/wh200720041/warehouse_simulation_toolkit
# slam-gmapping navigation


## INSTALL:

cd /home/jetson/ast_ws/src

git clone https://github.com/wh200720041/warehouse_simulation_toolkit.git

cd ..


catkin_make

source /home/jetson/ast_ws/devel/setup.bash

sudo apt-get install ros-noetic-hector-trajectory-server ros-noetic-slam-gmapping ros-noetic-navigation


## RUN:
roslaunch warehouse_simulation warehouse_simulation.launch

## WORK!!!



$ rostopic list 
/base_pose_ground_truth
/clicked_point
/clock
/initialpose
/cmd_vel

/gazebo/link_states
/gazebo/model_states
/gazebo/parameter_descriptions
/gazebo/parameter_updates
/gazebo/performance_metrics
/gazebo/set_link_state
/gazebo/set_model_state
/imu

/scan 			<<-- LaserScan

/map 			<<-- Map
/map_metadata
/map_updates

/move_base/NavfnROS/plan  <<-- Path

/move_base/TrajectoryPlannerROS/cost_cloud
/move_base/TrajectoryPlannerROS/global_plan
/move_base/TrajectoryPlannerROS/local_plan
/move_base/TrajectoryPlannerROS/parameter_descriptions
/move_base/TrajectoryPlannerROS/parameter_updates

/move_base/cancel
/move_base/current_goal
/move_base/feedback

/move_base/global_costmap/costmap 	<<-- Map
/move_base/global_costmap/costmap_updates
/move_base/global_costmap/footprint
/move_base/global_costmap/inflation_layer/parameter_descriptions
/move_base/global_costmap/inflation_layer/parameter_updates
/move_base/global_costmap/parameter_descriptions
/move_base/global_costmap/parameter_updates
/move_base/global_costmap/static_layer/parameter_descriptions
/move_base/global_costmap/static_layer/parameter_updates
/move_base/global_costmap/voxel_layer/clearing_endpoints
/move_base/global_costmap/voxel_layer/parameter_descriptions
/move_base/global_costmap/voxel_layer/parameter_updates
/move_base/global_costmap/voxel_layer/voxel_grid

/move_base/goal

/move_base/local_costmap/costmap	<<-- Map
/move_base/local_costmap/costmap_updates
/move_base/local_costmap/footprint
/move_base/local_costmap/inflation_layer/parameter_descriptions
/move_base/local_costmap/inflation_layer/parameter_updates
/move_base/local_costmap/parameter_descriptions
/move_base/local_costmap/parameter_updates
/move_base/local_costmap/voxel_layer/clearing_endpoints
/move_base/local_costmap/voxel_layer/parameter_descriptions
/move_base/local_costmap/voxel_layer/parameter_updates
/move_base/local_costmap/voxel_layer/voxel_grid

/move_base/parameter_descriptions
/move_base/parameter_updates
/move_base/recovery_status
/move_base/result
/move_base/status
/move_base_simple/goal
/odom
/pioneer/syscommand

/pioneer/trajectory 		<<-- Path
/pioneer3dx/joint_states
/rosout
/rosout_agg

/slam_gmapping/entropy
/tf
/tf_static

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
video-viewer --input-width=640 --input-height=480 csi://0

source /home/jetson/ast_ws/devel/setup.bash
roscore

roslaunch hls_lfcd_lds_driver hlds_laser.launch

roslaunch warehouse_simulation omni.launch


## WORK!!!



$ rostopic list
/base_pose_ground_truth
/clicked_point
/clock
/cmd_vel

/gazebo/link_states
/gazebo/model_states
/gazebo/parameter_descriptions
/gazebo/parameter_updates
/gazebo/performance_metrics
/gazebo/set_link_state
/gazebo/set_model_state

/initialpose

/map
/map_metadata

/move_base/NavfnROS/plan

/move_base/TrajectoryPlannerROS/cost_cloud
/move_base/TrajectoryPlannerROS/global_plan
/move_base/TrajectoryPlannerROS/local_plan
/move_base/TrajectoryPlannerROS/parameter_descriptions
/move_base/TrajectoryPlannerROS/parameter_updates

/move_base/cancel
/move_base/current_goal
/move_base/feedback

/move_base/global_costmap/costmap
/move_base/global_costmap/costmap_updates
/move_base/global_costmap/footprint
/move_base/global_costmap/inflation_layer/parameter_descriptions
/move_base/global_costmap/inflation_layer/parameter_updates
/move_base/global_costmap/parameter_descriptions
/move_base/global_costmap/parameter_updates
/move_base/global_costmap/static_layer/parameter_descriptions
/move_base/global_costmap/static_layer/parameter_updates
/move_base/global_costmap/voxel_layer/clearing_endpoints
/move_base/global_costmap/voxel_layer/parameter_descriptions
/move_base/global_costmap/voxel_layer/parameter_updates
/move_base/global_costmap/voxel_layer/voxel_grid

/move_base/goal
/move_base/local_costmap/costmap
/move_base/local_costmap/costmap_updates
/move_base/local_costmap/footprint
/move_base/local_costmap/inflation_layer/parameter_descriptions
/move_base/local_costmap/inflation_layer/parameter_updates
/move_base/local_costmap/parameter_descriptions
/move_base/local_costmap/parameter_updates
/move_base/local_costmap/voxel_layer/clearing_endpoints
/move_base/local_costmap/voxel_layer/parameter_descriptions
/move_base/local_costmap/voxel_layer/parameter_updates
/move_base/local_costmap/voxel_layer/voxel_grid
/move_base/parameter_descriptions
/move_base/parameter_updates
/move_base/recovery_status
/move_base/result
/move_base/status
/move_base_simple/goal

/odom
/omni/joint_states

/pioneer/syscommand
/pioneer/trajectory

/rosout
/rosout_agg
/rpms

/scan
/slam_gmapping/entropy

/tf
/tf_static

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<




## CHECK ERRORS:

roswtf

rosrun rqt_tf_tree rqt_tf_tree





## MAP SAVE
Saving your new map to disk: Once you have a satisfactory map in rviz, start a new terminal and run

$ rosrun map_server map_saver -f map
[ INFO] [1738515119.918850913]: Waiting for the map
[ INFO] [1738515120.121723762, 180.937000000]: Received a 480 X 480 map @ 0.050 m/pix
[ INFO] [1738515120.122733426, 180.938000000]: Writing map occupancy data to map.pgm
[ INFO] [1738515120.130246238, 180.945000000]: Writing map occupancy data to map.yaml
[ INFO] [1738515120.130629130, 180.945000000]: Done

This will save the current state of the map locally as map.pgm.

## WORK!




#ERROR IN LIDAR DATA ( all around... scale mismuch ???  )

# https://github.com/ros-perception/slam_gmapping/issues/79

If you're using the pointcloud_to_laserscan package to convert pointclouds from 3D pointclouds to 2D laser scan, try set the angle_increment to a higher value. I was using 0.003 for the pass 2 years but somehow this is the cause now. I change it to 0.005 and everything works again.


<!-- front lidar -->
  <gazebo reference="front_lidar_1">
    <sensor type="ray" name="front_hokuyo_sensor">
      <pose>0 0 0 0 0 0</pose>
      <visualize>true</visualize>
      <update_rate>40</update_rate>
      <ray>
        <scan>
          <horizontal>
            <samples>720</samples>
            <resolution>1</resolution>
            <min_angle>-1.570796</min_angle>
            <max_angle>1.570796</max_angle>
          </horizontal>
        </scan>
        <range>
          <min>0.10</min>
          <max>30.0</max>
          <resolution>0.01</resolution>
        </range>
        <noise>
          <type>gaussian</type>
          <!-- Noise parameters based on published spec for Hokuyo laser
               achieving "+-30mm" accuracy at range < 10m.  A mean of 0.0m and
               stddev of 0.01m will put 99.7% of samples within 0.03m of the true
               reading. -->
          <mean>0.0</mean>
          <stddev>0.01</stddev>
        </noise>
      </ray>
      <plugin name="gazebo_ros_front_hokuyo_controller" filename="libgazebo_ros_laser.so">
        <topicName>/scan</topicName>
        <frameName>front_lidar_1</frameName>
      </plugin>
    </sensor>
  </gazebo>





------------------------------------------------------------------------------------------
## ROS1 SSL_SLAM3
------------------------------------------------------------------------------------------
#SSL_SLAM
#Lightweight 3-D Localization and Mapping for Solid-State LiDAR (Intel Realsense L515 as an example)
This work provides a basic fusion framework that fuses LiDAR >>>>>>and IMU information <<<<<<<<< to improve the stability performance of SSL_SLA

# https://github.com/wh200720041/ssl_slam3




------------------------------------------------------------------------------------------
## ROS1 SSL_SLAM
------------------------------------------------------------------------------------------
#SSL_SLAM
#Lightweight 3-D Localization and Mapping for Solid-State LiDAR (Intel Realsense L515 as an example)

# https://github.com/wh200720041/ssl_slam

2. Prerequisites
2.1 Ubuntu and ROS
Ubuntu 64-bit 20.04.

ROS Noetic. ROS Installation

2.2. Ceres Solver
Follow Ceres Installation.

2.3. PCL
Follow PCL Installation.

Tested with 1.8.1

2.4 OctoMap
Follow OctoMap Installation.


## INSTALL:

source /home/jetson/ast_ws/devel/setup.bash
catkin_make


## ERROR:
CMake Error at /opt/ros/noetic/share/catkin/cmake/catkinConfig.cmake:83 (find_package):
  Could not find a package configuration file provided by "octomap_ros" with
  any of the following names:

    octomap_rosConfig.cmake
    octomap_ros-config.cmake


# FIX:
#https://github.com/OctoMap/octomap_ros
sudo apt-get install ros-noetic-octomap-ros
sudo apt-get install ros-noetic-octomap*

catkin_make

[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/ssl_slam/ssl_slam_odom_estimation_node
[100%] Built target ssl_slam_odom_estimation_node

## BUILD OK




## RUN:

roslaunch ssl_slam ssl_slam_mapping.launch

[FATAL] [1738501740.201300368]: Error opening file: /home/jetson/Downloads/L515_test.bag
[rosbag_play-1] process has died [pid 120347, exit code 1, cmd /opt/ros/noetic/lib/rosbag/play --clock /home/jetson/Downloads/L515_test.bag __name:=rosbag_play _

## FIX:
 
3.2 Download test rosbag
You may download our recorded data (5GB) if you dont have realsense L515, and by defult the file should be under home/user/Downloads unzip the file

cd ~/Downloads
unzip ~/Downloads/L515_test.zip


# if you would like to create the map at the same time, you can run
roslaunch ssl_slam ssl_slam_mapping.launch

## WWWOOORK!! 


# RUN:
# or create probability map
roslaunch ssl_slam ssl_slam_octo_mapping.launch

## ERROR:
[ WARN] [1738503574.064924122, 1596800383.264091872]: total points array 295
[ERROR] [1738503576.059344266, 1596800385.266681577]: PluginlibFactory: The plugin for class 'octomap_rviz_plugin/ColorOccupancyGrid' failed to load.  Error: According to the loaded plugin descriptions the class octomap_rviz_plugin/ColorOccupancyGrid with base class type rviz::Display does not exist. 



## FIX:
#https://github.com/OctoMap/octomap_rviz_plugins
sudo apt-get install ros-noetic-octomap-rviz-plugins


roslaunch ssl_slam ssl_slam_octo_mapping.launch



## WOOORRRK!!@111


$ rostopic list 

/camera/color/camera_info
/camera/color/image_raw
/camera/color/image_raw/mouse_click

/camera/depth/camera_info
/camera/depth/color/points 


/velodyne_points_filtered <<--PointCloud2

/laser_cloud_edge <<--Point Cloud edge
/laser_cloud_surf <<--Point Cloud surf

/octo_map <<-- ColorOccupancyGrig

/odom

/clicked_point
/clock
/initialpose


/map
/move_base_simple/goal


/rosout
/rosout_agg

/ssl_slam/syscommand
/ssl_slam/trajectory

/tf
/tf_static





------------------------------------------------------------------------------------------
ROS1 Slam Toolbox slam-toolbox
------------------------------------------------------------------------------------------

# https://github.com/SteveMacenski/slam_toolbox/tree/noetic-devel

# INSTALL:


source /home/jetson/ros_ws/devel/setup.bash
cd ~/ros_ws/src

git clone https://github.com/SteveMacenski/slam_toolbox.git

cd ..

# ROSDep will take care of the major things
rosdep install -q -y -r --from-paths src --ignore-src

executing command [sudo -H apt-get install -y -qq ros-noetic-tf2-sensor-msgs]
...
executing command [sudo -H apt-get install -y -qq libceres-dev]
...
Setting up libatlas3-base:arm64 (3.10.3-8ubuntu7) ...
Setting up libgoogle-glog0v5 (0.4.0-1build1) ...
Setting up libgoogle-glog-dev (0.4.0-1build1) ...
Setting up libceres1 (1.14.0-4ubuntu1.1) ...
Setting up libceres-dev (1.14.0-4ubuntu1.1) ...
Processing triggers for libc-bin (2.31-0ubuntu9.16) ...
#All required rosdeps installed successfully


catkin build



You don't have to compile slam_toolbox from source to use it, unless you want to modify the source code. You can install it with atp:

sudo apt update
sudo apt install ros-noetic-slam-toolbox
source /home/jetson/ros_ws/devel/setup.bash

After installing it, you can use the following launch file to use it:

ros2 launch slam_toolbox online_async_launch.py

You'll probably want to see it building in rviz2, you can add the /map topic once the file above is launched.

To save it, run

ros2 run nav2_map_server map_saver_cli -f [name of map]
If you want to see this done step by step, check out this video: https://youtu.be/JXnXnAXrYj8



------------------------------------------------------------------------------------------
## ROS1 Slam Toolbox
------------------------------------------------------------------------------------------

## INSTALL:

# https://github.com/SteveMacenski/slam_toolbox/issues/464 

# Chris Lofland over at Arlobot has seen the same issue as me when installing Slam Toolbox on Ubuntu 20.04 running ROS Noetic.

#However I think that the issue has been that we both have been using 
sudo apt install ros-noetic-slam-toolbox
 and not 
sudo apt install ros-noetic-slam-toolbox-rviz
" as you did.



## RUN:

source /opt/ros/noetic/setup.bash


roslaunch hls_lfcd_lds_driver hlds_laser.launch

roslaunch slam_toolbox localization.launch


/clicked_point
/initialpose

/map
/map_metadata

/move_base_simple/goal
/rosout
/rosout_agg
/scan
/slam_toolbox/feedback
/slam_toolbox/karto_graph_visualization
/slam_toolbox/karto_scan_visualization
/slam_toolbox/update
/slam_toolbox/update_full
/tf
/tf_static



{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{


source /home/jetson/ast_ws/devel/setup.bash

## RUN:

source ~/cart_ws/devel/setup.bash
roscore

roslaunch omni omni.launch
roslaunch omni omni_slam_toolbox.launch


file: "omni.launch"
"
<?xml version="1.0"?>
<launch>

<!-- Load robot description and start state publisher-->
   <!--param name="robot_description" textfile="/home/jetson/cart_ws/src/gbot_core/urdf/head_2d.urdf" /-->
   <param name="robot_description" command="$(find xacro)/xacro --inorder $(find omni)/urdf/omni.urdf.xacro"/>
   <node name="robot_state_publisher" pkg="robot_state_publisher" type="robot_state_publisher" />

  		
<!-- Start RPLIDAR sensor node which provides LaserScan data  -->
    <!--node name="rplidarNode" pkg="rplidar_ros" type="rplidarNode" output="screen">
        <param name="serial_port" type="string" value="/dev/ttyUSB0"/>
        <param name="serial_baudrate" type="int" value="115200"/>
        <param name="frame_id" type="string" value="laser"/>
        <param name="inverted" type="bool" value="false"/>
        <param name="angle_compensate" type="bool" value="true"/>
    </node-->
    
  <node pkg="hls_lfcd_lds_driver" type="hlds_laser_publisher" name="hlds_laser_publisher" output="screen">
    <param name="port" value="/dev/ttyUSB0"/>
    <param name="frame_id" value="laser"/>
  </node>	


  <!--hector_slam -->
    <!--arg name="geotiff_map_file_path" default="$(find hector_geotiff)/maps"/>

    <include file="$(find hector_mapping)/launch/mapping_default.launch"/>

    <include file="$(find hector_geotiff_launch)/launch/geotiff_mapper.launch">
        <arg name="trajectory_source_frame_name" value="scanmatcher_frame"/>
        <arg name="map_file_path" value="$(arg geotiff_map_file_path)"/>
    </include-->
		
<!-- Start Google Cartographer node with custom configuration file-->
    <!--node name="cartographer_node" pkg="cartographer_ros" type="cartographer_node" args="
          -configuration_directory
              $(find omni)/configuration_files
          -configuration_basename hlds_lidar_2d.lua" output="screen">
    </node-->

<!-- Additional node which converts Cartographer map into ROS occupancy grid map. Not used and can be skipped in this case -->
    <!--node name="cartographer_occupancy_grid_node" pkg="cartographer_ros" type="cartographer_occupancy_grid_node" args="-resolution 0.05">
        <remap from="map" to="map_cart"/>
    </node-->
  <node pkg="tf" type="static_transform_publisher" name="bfp_base_link_broadcaster" args="0 0 0 0 0 1 base_footprint base_link 100"/>
  <node pkg="tf" type="static_transform_publisher" name="odom_bfp_link_broadcaster" args="0 0 0 0 0 1 odom base_footprint 100"/>

  <node pkg="slam_toolbox" type="async_slam_toolbox_node" name="slam_toolbox" output="screen">
    <rosparam command="load" file="$(find slam_toolbox)/config/mapper_params_online_async.yaml" />
  </node>    
    
<!-- Start RViz with custom view -->
    <!--node pkg="rviz" type="rviz" name="show_rviz" args="-d $(find omni)/rviz/demo.rviz" /-->
    <node pkg="rviz" type="rviz" name="show_rviz" args="-d $(find omni)/rviz/demo_slam_toolbox.rviz" />
     
</launch>
"







{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{




------------------------------------------------------------------------------------------
# ROS1 RobotnikAutomation summit_xl_common RVIZ + GAZEBO SIM
------------------------------------------------------------------------------------------

# https://github.com/RobotnikAutomation/summit_xl_common

summit_xl_control

Summit XL OMNI (4 axes skid steering, 4 axes swerve drive)


https://gazebosim.org/docs/latest/tutorials/

## INSTALL 

source /home/jetson/ast_ws/devel/setup.bash


/home/jetson/ast_ws/src

## INSTALL DEPENDENCY

git clone https://github.com/RobotnikAutomation/mavros.git
git clone https://github.com/RobotnikAutomation/robotnik_msgs.git
git clone https://github.com/RobotnikAutomation/robotnik_sensors.git

sudo apt-get install ros-noetic-mavros ros-noetic-mavros-extras

#https://github.com/ros-controls/ros_control.git
sudo apt-get install ros-noetic-ros-control
sudo apt-get install ros-noetic-velocity-controllers

# sudo apt install ros-noetic-diff-drive-controller 
# sudo apt install ros-noetic-joint-state-controller

sudo apt-get install ros-noetic-twist-mux

##https://github.com/RobotnikAutomation/summit_xl_sim
#sudo apt-get install ros-noetic-summit-xl-sim


sudo ldconfig

git clone https://github.com/RobotnikAutomation/summit_xl_common.git


## BUILD

cd ..

catkin_make

## BUILD OK 
[100%] Linking CXX shared library /home/jetson/ast_ws/devel/lib/libmavros_plugins.so
[100%] Built target mavros_plugins


# RUN
source /home/jetson/ast_ws/devel/setup.bash
sudo ldconfig

roslaunch summit_xl_localization slam_gmapping.launch 

$ rostopic list

/clicked_point
/front_laser/scan
/initialpose
/map
/map_metadata
/move_base_simple/goal
/robot_odom
/rosout
/rosout_agg
/slam_gmapping/entropy
/tf
/tf_static

roslaunch summit_xl_description summit_xl_rviz.launch





# INSTALL ADDITION PACKAGES
# https://github.com/RobotnikAutomation/summit_xl_sim/tree/noetic-devel
sudo apt-get install -y python3-vcstool

vcs import --input https://raw.githubusercontent.com/RobotnikAutomation/summit_xl_sim/noetic-devel/repos/summit_xl_sim_devel.repos

rosdep install --from-paths src --ignore-src --skip-keys="summit_xl_robot_control marker_mapping robotnik_locator robotnik_pose_filter robotnik_gazebo_elevator" -y -r
... ~ 5 min

Setting up ros-noetic-force-torque-sensor-controller (0.22.0-1focal.20240913.201226) ...
Setting up ros-noetic-gripper-action-controller (0.22.0-1focal.20240913.201331) ...
Setting up ros-noetic-ackermann-steering-controller (0.22.0-1focal.20241220.160255) ...
Setting up ros-noetic-imu-sensor-controller (0.22.0-1focal.20240913.203224) ...
Setting up ros-noetic-ros-controllers (0.22.0-1focal.20241220.161536) ...
#All required rosdeps installed successfully



[100%] Built target robotnik_elevator_component
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/test_mavros/sitl_test_node
[100%] Built target sitl_test_node
[100%] Building CXX object robotnik_elevator_interface/robotnik_elevator_component/CMakeFiles/robotnik_fake_elevator_node.dir/src/robotnik_fake_elevator_node.cpp.o
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/kinova_driver/kinova_arm_driver
[100%] Built target kinova_arm_driver
[100%] Building CXX object robotnik_elevator_interface/elevator_controller_modbus/CMakeFiles/elevator_controller_interface_node.dir/src/elevator_controller_interface_node.cpp.o
[100%] Building CXX object robotnik_elevator_interface/elevator_controller_modbus/CMakeFiles/robotnik_elevator_component_modbus_node.dir/src/robotnik_elevator_component_modbus_node.cpp.o
[100%] Building CXX object robotnik_pad/robotnik_pad_plugins/CMakeFiles/robotnik_pad_plugins.dir/src/poi_plugin.cpp.o
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/rcomponent/rcomponent_diagnostic
[100%] Building CXX object trossen/interbotix_ros_core/interbotix_ros_xseries/interbotix_xs_sdk/CMakeFiles/xs_sdk.dir/src/xs_sdk_obj.cpp.o
[100%] Built target rcomponent_diagnostic
[100%] Building CXX object robotnik_pad/robotnik_pad_plugins/CMakeFiles/robotnik_pad_plugins.dir/src/ptz_plugin.cpp.o
[100%] Linking CXX shared library /home/jetson/ast_ws/devel/lib/librobotnik_pad.so
[100%] Built target robotnik_pad
[100%] Building CXX object robotnik_pad/robotnik_pad_plugins/CMakeFiles/robotnik_pad_plugins.dir/src/blkarc_plugin.cpp.o
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/robotnik_elevator_component/robotnik_fake_elevator_node
[100%] Built target robotnik_fake_elevator_node
[100%] Building CXX object robotnik_pad/robotnik_pad/CMakeFiles/robotnik_pad_node.dir/src/robotnik_pad_node.cpp.o
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/elevator_controller_modbus/robotnik_elevator_component_modbus_node
[100%] Built target robotnik_elevator_component_modbus_node
[100%] Linking CXX shared library /home/jetson/ast_ws/devel/lib/librobotnik_pad_plugins.so
[100%] Built target robotnik_pad_plugins
[100%] Building CXX object robotnik_pad/robotnik_pad_plugins/CMakeFiles/robotnik_pad_pluginlib.dir/src/generic_pad_plugins.cpp.o
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/elevator_controller_modbus/elevator_controller_interface_node
[100%] Built target elevator_controller_interface_node
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/kinova_driver/kinova_interactive_control
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/robotnik_pad/robotnik_pad_node
[100%] Built target robotnik_pad_node
[100%] Built target kinova_interactive_control
[100%] Linking CXX shared library /home/jetson/ast_ws/devel/lib/librobotnik_pad_pluginlib.so
[100%] Built target robotnik_pad_pluginlib
[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/interbotix_xs_sdk/xs_sdk
[100%] Built target xs_sdk


#### WOOOORK !!!!!!!!
## GAZEBO SIM WOOOOORK !!111

# rostopic list

/clicked_point
/clock
/diagnostics
/gazebo/link_states
/gazebo/model_states
/gazebo/parameter_descriptions
/gazebo/parameter_updates
/gazebo/performance_metrics
/gazebo/set_link_state
/gazebo/set_model_state
/robot/amcl/parameter_descriptions
/robot/amcl/parameter_updates
/robot/amcl_pose
/robot/cmd_vel
/robot/docker/cmd_vel
/robot/front_laser/scan
/robot/front_rgbd_camera/depth/camera_info
/robot/front_rgbd_camera/depth/image_raw
/robot/front_rgbd_camera/depth/points
/robot/front_rgbd_camera/parameter_descriptions
/robot/front_rgbd_camera/parameter_updates
/robot/front_rgbd_camera/rgb/camera_info
/robot/front_rgbd_camera/rgb/image_raw
/robot/front_rgbd_camera/rgb/image_raw/compressed
/robot/front_rgbd_camera/rgb/image_raw/compressed/parameter_descriptions
/robot/front_rgbd_camera/rgb/image_raw/compressed/parameter_updates
/robot/front_rgbd_camera/rgb/image_raw/compressedDepth
/robot/front_rgbd_camera/rgb/image_raw/compressedDepth/parameter_descriptions
/robot/front_rgbd_camera/rgb/image_raw/compressedDepth/parameter_updates
/robot/front_rgbd_camera/rgb/image_raw/theora
/robot/front_rgbd_camera/rgb/image_raw/theora/parameter_descriptions
/robot/front_rgbd_camera/rgb/image_raw/theora/parameter_updates
/robot/gps/fix
/robot/gps/fix/position/parameter_descriptions
/robot/gps/fix/position/parameter_updates
/robot/gps/fix/status/parameter_descriptions
/robot/gps/fix/status/parameter_updates
/robot/gps/fix/velocity/parameter_descriptions
/robot/gps/fix/velocity/parameter_updates
/robot/gps/fix_velocity
/robot/imu/data
/robot/imu/data_raw
/robot/imu/rpy/filtered
/robot/initialpose
/robot/joint_states
/robot/map
/robot/map_metadata
/robot/map_updates
/robot/move/cmd_vel
/robot/move_base/NavfnROS/plan
/robot/move_base/TebLocalPlannerROS/local_plan
/robot/move_base/cmd_vel
/robot/move_base/current_goal
/robot/move_base/global_costmap/costmap
/robot/move_base/global_costmap/costmap_updates
/robot/move_base/goal
/robot/move_base/local_costmap/costmap
/robot/move_base/local_costmap/costmap_updates
/robot/move_base/recovery_status
/robot/move_base_simple/goal
/robot/pad_teleop/cmd_vel
/robot/particlecloud
/robot/rear_laser/scan
/robot/robotnik_base_control/cmd_vel
/robot/robotnik_base_control/emergency_stop
/robot/robotnik_base_control/odom
/robot/robotnik_base_hw/emergency_stop
/robot/twist_marker
/robot/urcap_command_bridge/cmd_vel
/robot/web_teleop/cmd_vel
/rosout
/rosout_agg
/tf
/tf_static



4. Launch Summit XL simulation (1 robot by default, up to 3 robots):
Summit XL:

roslaunch summit_xl_sim_bringup summit_xl_complete.launch


Summit XL with Trossen Arm

roslaunch summit_xl_sim_bringup summit_xl_complete.launch default_xacro:=summit_xl_tix_std.urdf.xacro launch_arm_a:=true arm_manufacturer_a:=trossen arm_model_a:=vx300s


Launch moveit to plan trajectories:
ROS_NAMESPACE=robot roslaunch summit_xl_vx300s_moveit_config demo.launch


Summit XL with Kinova Arm

roslaunch summit_xl_sim_bringup summit_xl_complete.launch default_xacro:=summit_xl_gen_std.urdf.xacro launch_arm_a:=true arm_manufacturer_a:=kinova arm_model_a:=j2s7s300 amcl_and_mapserver_a:=false move_base_robot_a:=false

Note: in this configuration the robot has not laser, therefore the amcl is turned off. When Rviz is opened, change robot_map to robot_odom in fixed_frame in order to visualize the robot.


# or Summit XL Steel:
roslaunch summit_xl_sim_bringup summit_xls_complete.launch
## WORK

$ rostopic list


/clicked_point
/clock
/diagnostics
/gazebo/link_states
/gazebo/model_states
/gazebo/parameter_descriptions
/gazebo/parameter_updates
/gazebo/performance_metrics
/gazebo/set_link_state
/gazebo/set_model_state
/robot/amcl/parameter_descriptions
/robot/amcl/parameter_updates
/robot/amcl_pose
/robot/cmd_vel
/robot/docker/cmd_vel
/robot/front_laser/scan
/robot/front_rgbd_camera/depth/camera_info
/robot/front_rgbd_camera/depth/image_raw
/robot/front_rgbd_camera/depth/points
/robot/front_rgbd_camera/parameter_descriptions
/robot/front_rgbd_camera/parameter_updates
/robot/front_rgbd_camera/rgb/camera_info
/robot/front_rgbd_camera/rgb/image_raw
/robot/front_rgbd_camera/rgb/image_raw/compressed
/robot/front_rgbd_camera/rgb/image_raw/compressed/parameter_descriptions
/robot/front_rgbd_camera/rgb/image_raw/compressed/parameter_updates
/robot/front_rgbd_camera/rgb/image_raw/compressedDepth
/robot/front_rgbd_camera/rgb/image_raw/compressedDepth/parameter_descriptions
/robot/front_rgbd_camera/rgb/image_raw/compressedDepth/parameter_updates
/robot/front_rgbd_camera/rgb/image_raw/theora
/robot/front_rgbd_camera/rgb/image_raw/theora/parameter_descriptions
/robot/front_rgbd_camera/rgb/image_raw/theora/parameter_updates
/robot/imu/data
/robot/imu/data_raw
/robot/imu/rpy/filtered
/robot/initialpose
/robot/joint_states
/robot/map
/robot/map_metadata
/robot/map_updates
/robot/move/cmd_vel
/robot/move_base/GlobalPlanner/parameter_descriptions
/robot/move_base/GlobalPlanner/parameter_updates
/robot/move_base/GlobalPlanner/plan
/robot/move_base/GlobalPlanner/potential
/robot/move_base/NavfnROS/plan
/robot/move_base/TebLocalPlannerROS/costmap_converter/CostmapToPolygonsDBSMCCH/parameter_descriptions
/robot/move_base/TebLocalPlannerROS/costmap_converter/CostmapToPolygonsDBSMCCH/parameter_updates
/robot/move_base/TebLocalPlannerROS/global_plan
/robot/move_base/TebLocalPlannerROS/local_plan
/robot/move_base/TebLocalPlannerROS/obstacles
/robot/move_base/TebLocalPlannerROS/parameter_descriptions
/robot/move_base/TebLocalPlannerROS/parameter_updates
/robot/move_base/TebLocalPlannerROS/teb_feedback
/robot/move_base/TebLocalPlannerROS/teb_markers
/robot/move_base/TebLocalPlannerROS/teb_poses
/robot/move_base/TebLocalPlannerROS/via_points
/robot/move_base/cancel
/robot/move_base/cmd_vel
/robot/move_base/current_goal
/robot/move_base/feedback
/robot/move_base/global_costmap/costmap
/robot/move_base/global_costmap/costmap_updates
/robot/move_base/global_costmap/footprint
/robot/move_base/global_costmap/inflation_layer/parameter_descriptions
/robot/move_base/global_costmap/inflation_layer/parameter_updates
/robot/move_base/global_costmap/parameter_descriptions
/robot/move_base/global_costmap/parameter_updates
/robot/move_base/global_costmap/static_map_layer/parameter_descriptions
/robot/move_base/global_costmap/static_map_layer/parameter_updates
/robot/move_base/goal
/robot/move_base/local_costmap/costmap
/robot/move_base/local_costmap/costmap_updates
/robot/move_base/local_costmap/footprint
/robot/move_base/local_costmap/inflation_layer/parameter_descriptions
/robot/move_base/local_costmap/inflation_layer/parameter_updates
/robot/move_base/local_costmap/obstacle_laser_layer/parameter_descriptions
/robot/move_base/local_costmap/obstacle_laser_layer/parameter_updates
/robot/move_base/local_costmap/parameter_descriptions
/robot/move_base/local_costmap/parameter_updates
/robot/move_base/parameter_descriptions
/robot/move_base/parameter_updates
/robot/move_base/recovery_status
/robot/move_base/result
/robot/move_base/status
/robot/move_base_simple/goal
/robot/pad_teleop/cmd_vel
/robot/particlecloud
/robot/rear_laser/scan
/robot/robotnik_base_control/cmd_vel


/robot/robotnik_base_control/emergency_stop
/robot/robotnik_base_control/odom
/robot/robotnik_base_hw/emergency_stop
/robot/twist_marker
/robot/urcap_command_bridge/cmd_vel
/robot/web_teleop/cmd_vel
/rosout
/rosout_agg
/tf
/tf_static


## CHANGE THE TOPIC from "rostopic pub"

# The topic /cmd_vel uses the geometry_msgs/Twist type (64 bits) of the geometry_msgs message library
rostopic info /robot/robotnik_base_control/cmd_vel


## https://forums.developer.nvidia.com/t/how-to-set-velocities-of-a-mobile-robot-with-cmd-vel/184639/3
rostopic pub -r 10 /robot/robotnik_base_control/cmd_vel geometry_msgs/Twist '{linear:  {x: 1.0, y: 0.0, z: 0.0}, angular: {x: 0.0,y: 0.0,z: 1.0}}'


## https://wiki.ros.org/Robots/TIAGo/Tutorials/motions/cmd_vel
# This expresses velocity in free space broken into its linear and angular parts.
# which is of type geometry_msgs/Twist. A twist is composed of:

geometry_msgs/Vector3 linear
geometry_msgs/Vector3 angular

# Moving forward and backward
rostopic pub /robot/robotnik_base_control/cmd_vel geometry_msgs/Twist -r 3 -- '[0.5,0.0,0.0]' '[0.0, 0.0, 0.0]'

rostopic pub /robot/robotnik_base_control/cmd_vel geometry_msgs/Twist "linear:
  x: 0.5
  y: 0.0
  z: 0.0
angular:
  x: 0.0
  y: 0.0
  z: 0.0" -r 3

# Turning left an right
rostopic pub /mobile_base_controller/cmd_vel geometry_msgs/Twist -r 3 -- '[0.0,0.0,0.0]' '[0.0, 0.0, 0.3]'






--------------------------------------
Jetson orin NX 16G | MY_IP: 192.168.3.19
--------------------------------------
ROS1 core 
source /opt/ros/noetic/setup.bash



>>>>>>>>>> catkin_make
source /home/jetson/ast_ws/devel/setup.bash
roscd
jetson@ubuntu:~/ast_ws/devel$



>>>>>>>>>> catkin build
source /home/jetson/ros_ws/devel/setup.bash
roscd
jetson@ubuntu:~/ros_ws/devel


jetson@ubuntu:source /home/jetson/yahboomcar_ws/devel/setup.bash
jetson@ubuntu:~$roscd
jetson@ubuntu:~/yahboomcar_ws/devel$

jetson@ubuntu:~/yahboomcar_ws/devel$ source /opt/ros/noetic/setup.bash
jetson@ubuntu:~/yahboomcar_ws/devel$ roscd
jetson@ubuntu:/opt/ros/noetic$

jetson@ubuntu:/opt/ros/noetic$ source /home/jetson/catkin_ws/devel/setup.bash
jetson@ubuntu:/opt/ros/noetic$ roscd
jetson@ubuntu:~/catkin_ws/devel$ 

------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------


rosrun image_view image_view image:=/extract_image_channel_r/output _do_dynamic_scaling:=true


## https://stackoverflow.com/questions/69190238/how-can-i-publish-an-image-topic-using-opencv-on-ros-without-cvbridge

>>> from sensor_msgs.msg import Image
>>> Image.__slots__
['header', 'height', 'width', 'encoding', 'is_bigendian', 'step', 'data']
>>> Image._slot_types
['std_msgs/Header', 'uint32', 'uint32', 'string', 'uint8', 'uint32', 'uint8[]']





------------------------------------------------------------------------------------------
## ROS1 image_publisher
------------------------------------------------------------------------------------------
# https://wiki.ros.org/image_publisher

First, to stream some image data for the inferencing node to process, open another terminal and start an image_publisher, which loads a specified image from disk. We tell it to load one of the test images that come with jetson-inference, but you can substitute your own images here as well:


rosrun image_publisher image_publisher __name:=image_publisher ~/jetson-inference/data/images/orange_0.jpg

$ rosrun image_publisher image_publisher __name:=image_publisher cs_ball.png
[ INFO] [1738255817.169207336]: File name for publishing image is : cs_ball.png
[ INFO] [1738255817.171364520]: Flip horizontal image is : false
[ INFO] [1738255817.171796245]: Flip flip_vertical image is : false


$ rostopic list
/image_publisher/camera_info
/image_publisher/image_raw
/image_publisher/image_raw/compressed
/image_publisher/image_raw/compressed/parameter_descriptions
/image_publisher/image_raw/compressed/parameter_updates
/image_publisher/image_raw/compressedDepth
/image_publisher/image_raw/compressedDepth/parameter_descriptions
/image_publisher/image_raw/compressedDepth/parameter_updates
/image_publisher/image_raw/theora
/image_publisher/image_raw/theora/parameter_descriptions
/image_publisher/image_raw/theora/parameter_updates
/image_publisher/parameter_descriptions
/image_publisher/parameter_updates
/rosout
/rosout_agg


## WORK 





------------------------------------------------------------------------------------------
# ROS notice ERROR 
------------------------------------------------------------------------------------------
File "ros_image_subs_publish.py", line 3, in <module>
    from cv_bridge import CvBridge
  File "/opt/ros/noetic/lib/python3/dist-packages/cv_bridge/__init__.py", line 6, in <module>
    from cv_bridge.boost.cv_bridge_boost import cvtColorForDisplay, getCvType
SystemError: initialization of cv_bridge_boost raised unreported exception

## FIX:

I had the same issue on Jetson Xavier on ubuntu 20.04 while it was working well on amd PC with same linux version.

Importing cv2 before cv_bridge solved it for me.

import cv2
from cv_bridge import CvBridge, CvBridgeError

## WORK !!






------------------------------------------------------------------------------------------
# Install OpenCV on Jetson Orin Nano with CUDA support
------------------------------------------------------------------------------------------
# https://qengineering.eu/install-opencv-on-orin-nano.html








## https://forums.developer.nvidia.com/t/ros-packages-for-working-with-cameras-on-jetson-nano/163781/3

$ v4l2-ctl --device=/dev/video0 --list-formats-ext
ioctl: VIDIOC_ENUM_FMT
	Type: Video Capture

	[0]: 'RG10' (10-bit Bayer RGRG/GBGB)
		Size: Discrete 3280x2464
			Interval: Discrete 0.048s (21.000 fps)
		Size: Discrete 3280x1848
			Interval: Discrete 0.036s (28.000 fps)
		Size: Discrete 1920x1080
			Interval: Discrete 0.033s (30.000 fps)
		Size: Discrete 1640x1232
			Interval: Discrete 0.033s (30.000 fps)
		Size: Discrete 1280x720
			Interval: Discrete 0.017s (60.000 fps)



------------------------------------------------------------------------------------------
# ROS video_stream_opencv 
------------------------------------------------------------------------------------------
# https://github.com/peter-moran/jetson_csi_cam

## NOT WORK





------------------------------------------------------------------------------------------
# ROS jetson csi camera 
------------------------------------------------------------------------------------------
# https://github.com/peter-moran/jetson_csi_cam


1. Download jetson_csi_cam
Clone this repository into you catkin_workspace.

cd ~/catkin_workspace/src
git clone https://github.com/peter-moran/jetson_csi_cam.git 
2. Install gscam with gstreamer-1.0 support
Clone gscam into your catkin_workspace.

cd ~/catkin_workspace/src
git clone https://github.com/ros-drivers/gscam.git
Then edit ./gscam/Makefile and add the CMake flag -DGSTREAMER_VERSION_1_x=On to the first line of the file, so that it reads:

EXTRA_CMAKE_FLAGS = -DUSE_ROSBUILD:BOOL=1 -DGSTREAMER_VERSION_1_x=On
While this flag is only necessary if you have both gstreamer-0.1 and gstreamer-1.0 installed simultaneously, it is good practice to include.

3. Build everything
Now we build and register gscam and jetson_csi_cam in ROS.

cd ~/catkin_workspace
catkin_make
source ~/.bashrc
At this point everything should be ready to go.




## RUN
roslaunch jetson_csi_cam jetson_csi_cam2.launch width:=1280 height:=720 fps:=30
roslaunch jetson_csi_cam jetson_csi_cam.launch



## ERROR:
(gscam:28438): GStreamer-CRITICAL **: 15:03:32.840: gst_element_make_from_uri: assertion 'gst_uri_is_valid (uri)' failed


## FIX:

I also got this error and didn't see any image topic. I solved it by replacing the definition of GSCAM_CONFIG in jetson_csi_cam.launch with the following lines:

  <env name="GSCAM_CONFIG" value="nvarguscamerasrc sensor-id=$(arg sensor_id) ! video/x-raw(memory:NVMM),
    width=(int)$(arg width), height=(int)$(arg height), format=(string)I420, framerate=(fraction)$(arg fps)/1 ! 
    nvvidconv flip-method=2 ! video/x-raw, format=(string)BGRx ! videoconvert" />
It's basically nvarguscamerasrc instead of nvcamerasrc and removing the trailing video/x-raw, format=(string)BGR.

## NOT WORK.......






Hello! Happy Chinese New Year! )

Can you confirm that my Jetson orin NX can run Jetpack 6.2?
This is a photo of my Jetson booting up.
It doesn't look like it from the UEFI firmware version...
5.0-35550185 instead of 36.3.0-gcid-36106755 as described in the manual.



Можете вы подтвердить что на мой Jetson orin NX можно  поставить Jetpack 6.2?
Это фото загрузки моего Jetson.
По UEFI firmware version это не похоже...
5.0-35550185 вместо 36.3.0-gcid-36106755, как описано в руководстве.


https://github.com/introlab/rtabmap/wiki/Kinect-mapping





------------------------------------------------------------------------------------------
# ROS SLAM with ASTRA CAMERA
------------------------------------------------------------------------------------------

Sergey Dorodnicov edited this page on Jan 23, 2019 · 6 revisions

# SLAM with RealSense™ D435i camera on ROS:
The RealSense™ D435i is equipped with a built in IMU. Combined with some powerful open source tools, it's possible to achieve the tasks of mapping and localization.

# There are 4 main nodes to the process:

realsense2_camera
imu_filter_madgwick
rtabmap_ros
robot_localization

# Installation:
The first thing to do is to install the components:
realsense2_camera: Follow the installation guide in: https://github.com/intel-ros/realsense.

imu_filter_madgwick: sudo apt-get install ros-noetic-imu-filter-madgwick
rtabmap_ros: sudo apt-get install ros-noetic-rtabmap-ros
robot_localization: sudo apt-get install ros-noetic-robot-localization

Running:
Hold the camera steady with a clear view and run the following command:

roslaunch realsense2_camera opensource_tracking.launch
Wait a little for the system to fix itself.








__________________

https://github.com/engcang/SLAM-application

SLAM-application: installation and test
3D, single-LiDAR without IMU (or optionally with IMU): LeGO-LOAM, KISS-ICP, DLO, CT-ICP, and GenZ-ICP
3D, single-LiDAR with IMU: LIO-SAM, LVI-SAM, FAST-LIO2, Faster-LIO, VoxelMap, R3LIVE, PV-LIO, SLAMesh, ImMesh, iG-LIO, and SR-LIO
3D, multi-LiDARs: FAST-LIO-MULTI, M-LOAM, LOCUS2.0, SLICT1.0, and MA-LIO


_________________________________________________
## INSTALL ROS CORE
## https://wiki.ros.org/noetic/Installation/Ubuntu
sudo apt install ros-noetic-desktop-full
_________________________________________________
## https://answers.ros.org/question/342505/

sudo apt install ros-${ROS_DISTRO}-ros-core

## WILL INSTALL THIS:
roscd -> ros-noetic-rosbash
roslaunch -> ros-noetic-roslaunch
roscore -> ros-noetic-roslaunch.
rostopic-> ros-noetic-rostopic

sudo apt-get install ros-noetic-image-view
image_view






$ sudo apt install ros-noetic-rqt
After that make sure to run source ~/.bashrc. Your .bashrc file should already contain the command line source /opt/ros/noetic/setup.bash.

Launch rqt_graph
First, you need to have a ROS master running in your environment.
After having launched roscore, start rqt graph with:

$ rosrun rqt_graph rqt_graph
You can also simply use:

$ rqt_graph

-------------------------------------------------------------------
## ERROR rviz shows that there is no transformation from base_link 
-------------------------------------------------------------------

I'm trying to use this package with ROS Noetic (roslaunch lio_sam run.launch). However, my rviz shows that there is no transformation from base_link (chassis_link, imu_link, navast_link, and velodyne_link) to frame map. The base_link status is fixed frame does not exist and the Global Status is unknown frame map.

I think it is a very basic problem. I have read all the issues but could not find a similar problem faced by anyone else :(


# FIX:

Modify Fixed Frame in Global Options and map to base_link.
Different datasets modify parameters in config/params.yaml.
For example, Rotation dataset's imu topic needs to be changed to imuTopic: "imu_correct", 
and "extrinsicRot" and "extrinsicRPY" need to be set to rotation matrix.



_____________________________________________________________________________
## ROS Building a Map in Simulation
_____________________________________________________________________________
## https://wiki.ros.org/pr2_simulator/Tutorials/BuildingAMapInSimulation


## Installation
For this tutorial, you need to install the 'pr2all' variant of ROS (see installation instructions). On Ubuntu, this means:

sudo apt-get install ros-noetic-pr2-apps

sudo apt-get install ros-noetic-slam_gmapping



sudo apt-get install ros-noetic-map-server


Are you sure you have set up everything correctly for using simulation time (so the timestamps from the bagfile are used)? Also see the Clock documentation regarding this. You have start a roscore, set the parameter:

!!!
$ rosparam set use_sim_time true
!!!

#then start anything else you need and then play your bagfile with the --clock option:
rosbag play --clock  '/home/jetson/Downloads/basic_localization_stage.bag'

#Wait for rosbag to finish and exit.

#As for the rosconsole logger question, you can use the rxconsole tool to set different log levels for different nodes and get more debug output.

rosrun map_server map_saver -f my_map



------------------------------------------------------------------------------------------
# INSTALL & RUN ROS ASTRA PRO 3D CAMERA WRAPPER
------------------------------------------------------------------------------------------
## https://github.com/orbbec/ros_astra_camera


----------- NOT NEED -----------
# Install dependencies (be careful with your ROS distribution)

# Assuming you have sourced the ros environment, same below
sudo apt install libgflags-dev  ros-$ROS_DISTRO-image-geometry ros-$ROS_DISTRO-camera-info-manager\
ros-$ROS_DISTRO-image-transport ros-$ROS_DISTRO-image-publisher  libusb-1.0-0-dev libeigen3-dev
ros-$ROS_DISTRO-backward-ros libdw-dev
Install libuvc.

git clone https://github.com/libuvc/libuvc.git
cd libuvc
mkdir build && cd build
cmake .. && make -j4
sudo make install
sudo ldconfig
----------- NOT NEED -----------


# Create a ros workspace( if you don't have one).
mkdir -p ~/ast_ws/src

# Clone code from github.
 cd ~/ast_ws/src
 git clone https://github.com/orbbec/ros_astra_camera.git



# Build
cd ~/ast_ws
catkin_make

## ERROR

[100%] Linking CXX executable /home/jetson/ast_ws/devel/lib/astra_camera/astra_camera_node
/usr/bin/ld: /home/jetson/ast_ws/devel/lib/libastra_camera.so: undefined reference to `cv::Mat::Mat()'
/usr/bin/ld: /home/jetson/ast_ws/devel/lib/libastra_camera.so: undefined reference to `cv::Mat::Mat(int, int, int, cv::Scalar_<double> const&)'
/usr/bin/ld: /home/jetson/ast_ws/devel/lib/libastra_camera.so: undefined reference to `cv::Mat::Mat(int, int, int)'
collect2: error: ld returned 1 exit status
make[2]: *** [ros_astra_camera/CMakeFiles/astra_camera_node.dir/build.make:282: /home/jetson/ast_ws/devel/lib/astra_camera/astra_camera_node] Error 1
make[1]: *** [CMakeFiles/Makefile2:3223: ros_astra_camera/CMakeFiles/astra_camera_node.dir/all] Error 2
make: *** [Makefile:146: all] Error 2
Invoking "make -j8 -l8" failed

## FIX%

sudo ldconfig

In case people encountered the same problem as me,
I found a hack to solve this error.

add an extra vision_opencv package in src folder then catkin_make again

git clone https://github.com/MartinNievas/vision_opencv.git
cd vision_opencv/ 
git checkout compile_oCV4
cd ../
cd ../
catkin_make

##### THIS WORKING !!!!!!!!


....

## ANOTHER ERROR

CMake Error at /usr/lib/aarch64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake:117 (find_package):
  Could not find a package configuration file provided by "boost_python3"
  (requested version 1.71.0) with any of the following names:

    boost_python3Config.cmake
    boost_python3-config.cmake

  Add the installation prefix of "boost_python3" to CMAKE_PREFIX_PATH or set
  "boost_python3_DIR" to a directory containing one of the above files.  If
  "boost_python3" provides a separate development package or SDK, be sure it
  has been installed.
Call Stack (most recent call first):
  /usr/lib/aarch64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake:182 (boost_find_component)
  /opt/cmake-3.31.4-linux-aarch64/share/cmake-3.31/Modules/FindBoost.cmake:610 (find_package)
  vision_opencv/cv_bridge/CMakeLists.txt:11 (find_package)


-- Configuring incomplete, errors occurred!
Invoking "cmake" failed

## FIX:

sudo ldconfig

I just bumped into this as well on noetic w/ Ubuntu 20.04. In my case, I had to edit vision_opencv/cv_bridge/CMakeLists.txt as:

if(NOT ANDROID)
  find_package(PythonLibs)
  #  if(PYTHONLIBS_VERSION_STRING VERSION_LESS 3)
    find_package(Boost REQUIRED python)
    #else()
    #find_package(Boost REQUIRED python3)
    #endif()
else()
find_package(Boost REQUIRED)
endif()
i.e. basically comment out the test for checking for python2.


##### THIS WORKING !!!!!!!!
##### THIS WORKING !!!!!!!!
##### THIS WORKING !!!!!!!!


$ catkin_make
........
[100%] Built target astra_camera_node


# Install udev rules.
cd ~/ast_ws
source ./devel/setup.bash
roscd astra_camera

./scripts/create_udev_rules
sudo udevadm control --reload && sudo  udevadm trigger


## RUN

Start the camera

In terminal 1
#source ./devel/setup.bash
source /home/jetson/ast_ws/devel/setup.bash

## roslaunch astra_camera astra.launch
## roslaunch astra_camera astra_pro_plus.launch
roslaunch astra_camera astra_pro.launch
roslaunch astra_camera astra_pro_640x480.launch

WORK


$ rostopic list
/camera/color/camera_info
/camera/color/image_raw

/camera/depth/camera_info
/camera/depth/image_raw
/camera/depth/points

/camera/ir/camera_info
/camera/ir/image_raw

/rosout
/rosout_agg

/tf
/tf_static



rosrun image_view image_view image:=/camera/color/image_raw
WORK

rosrun image_view image_view image:=/camera/depth/image_raw
WORK

rosrun image_view image_view image:=/camera/ir/image_raw
WORK


In terminal 2
source /home/jetson/ast_ws/devel/setup.bash
rviz
WORK




rosservice call /camera/set_uvc_auto_white_balance  "{data: false}" //true



--


[ INFO] [1737740783.608747246]: Start device 
[ WARN] [1737740783.694155817]: Video mode Resolution :1280x720@30Hz
format PIXEL_FORMAT_DEPTH_1_MM is not supported. 
[ WARN] [1737740783.694228843]: Default video mode Resolution :1280x720@0Hz
format PIXEL_FORMAT_DEPTH_1_MMis not supported. Stream will be disabled.
[ WARN] [1737740783.694250348]: Supported video modes: 
[ WARN] [1737740783.694265996]: Resolution :160x120@30Hz
format PIXEL_FORMAT_DEPTH_1_MM
[ WARN] [1737740783.694278861]: Resolution :160x120@30Hz
format PIXEL_FORMAT_DEPTH_100_UM
[ WARN] [1737740783.694293421]: Resolution :320x240@30Hz
format PIXEL_FORMAT_DEPTH_1_MM
[ WARN] [1737740783.694312654]: Resolution :320x240@30Hz
format PIXEL_FORMAT_DEPTH_100_UM
[ WARN] [1737740783.694330830]: Resolution :640x480@30Hz
format PIXEL_FORMAT_DEPTH_1_MM
[ WARN] [1737740783.694347023]: Resolution :640x480@30Hz
format PIXEL_FORMAT_DEPTH_100_UM
[ WARN] [1737740783.694363439]: Resolution :1280x1024@7Hz
format PIXEL_FORMAT_DEPTH_1_MM
[ WARN] [1737740783.694393744]: Resolution :1280x1024@7Hz
format PIXEL_FORMAT_DEPTH_100_UM


## Launch parameters
connection_delay:The delay time for reopening the device in milliseconds. Some devices may take longer to initialize, such as the Astra mini. Reopening the device immediately may cause firmware crashes when hot plugging.
enable_point_cloud: Whether to enable point cloud..
enable_point_cloud_xyzrgb,:Whether to enable RGB point cloud.
enable_d2c_viewer: Publish D2C overlay image(For testing only).
device_num: The number of devices. You need to specify the number of devices when using multiple cameras.
enable_reconfigure, Whether to enable ROS dynamic configuration changes, set to false means that the Astra.cfg configuration will not take effect. This is recommended for testing purposes only. Turn it off when in use. use. .
color_width， color_height， color_fps: Color stream resolution and frame rate.
ir_width， ir_height， ir_fps:IR stream resolution and frame rate.
depth_width， depth_height， depth_fps: Depth stream resolution and frame rate.
enable_color: Whether to enable RGB camera. This parameter has no effect when the RGB camera is using the UVC protocol.
enable_depth : Whether to enable the depth camera.
enable_ir: Whether to enable the IR camera.
depth_align: Enables hardware depth to color alignment, which is required when the RGB point cloud is enabled.
depth_scale: Depth image zoom scale. For example, setting it to 2 means aligning a depth image of size 320x240 to an RGB image of size 640x480.
color_roi_x， color_roi_y， color_roi_width， color_roi_height:: Whether to crop RGB images. The default is -1, which is only used when the RGB resolution is greater than the depth resolution and needs to be aligned. For example, if you need to align a depth image of size 640x400 to an RGB image of size 640x480, you need to set color_roi_x to 0, color_roi_y to 0, color_roi_width to 640, and color_roi_height to 400. This will crop the top 400 pixels of the RGB image with a corresponding depth ROI.
color_depth_synchronization，Enable synchronization of RGB with depth
use_uvc_camera: If the RGB camera is using the UVC protocol, set this parameter to true. UVC is the protocol that currently includes Dabai, Dabai_dcw, and so on.
uvc_product_id:PID of the UVC camera.
uvc_camera_format:Image format for the UVC camera.
uvc_retry_count : Sometimes the UVC protocol camera does not reconnect successfully when hot plugging, requiring many retries.
enable_publish_extrinsic Enable publishing camera extrinsic.
oni_log_level: Log levels for OpenNI: verbose, info, warning, error, or none.
oni_log_to_console, Whether to output OpenNI logs to the console.
oni_log_to_file:Whether to output OpenNI logs to a file. By default, it will be saved in the Log folder under the path of the currently running program.
For special customer requirements:
enable_keep_alive, Whether to send heartbeat packets to the firmware. This is not enabled by default.
keep_alive_interval, The time interval in seconds between sending heartbeat packets.


$ roslaunch astra_camera astra_pro.launch
## FULL LOG 

... logging to /home/jetson/.ros/log/dd2397f8-da75-11ef-8a09-3c6d66115ee2/roslaunch-ubuntu-36553.log
Checking log directory for disk usage. This may take a while.
Press Ctrl-C to interrupt
Done checking log file disk usage. Usage is <1GB.

started roslaunch server http://ubuntu:38055/

SUMMARY
========

PARAMETERS
 * /camera/camera/camera_name: camera
 * /camera/camera/color_depth_synchronization: False
 * /camera/camera/color_format: RGB
 * /camera/camera/color_fps: 30
 * /camera/camera/color_height: 480
 * /camera/camera/color_info_uri: 
 * /camera/camera/color_roi_height: -1
 * /camera/camera/color_roi_width: -1
 * /camera/camera/color_roi_x: -1
 * /camera/camera/color_roi_y: -1
 * /camera/camera/color_width: 640
 * /camera/camera/connection_delay: 100
 * /camera/camera/depth_align: False
 * /camera/camera/depth_format: Y11
 * /camera/camera/depth_fps: 30
 * /camera/camera/depth_height: 480
 * /camera/camera/depth_roi_height: -1
 * /camera/camera/depth_roi_width: -1
 * /camera/camera/depth_roi_x: -1
 * /camera/camera/depth_roi_y: -1
 * /camera/camera/depth_scale: 1
 * /camera/camera/depth_width: 640
 * /camera/camera/device_num: 1
 * /camera/camera/enable_color: True
 * /camera/camera/enable_d2c_viewer: False
 * /camera/camera/enable_depth: True
 * /camera/camera/enable_ir: True
 * /camera/camera/enable_keep_alive: False
 * /camera/camera/enable_point_cloud: True
 * /camera/camera/enable_point_cloud_xyzrgb: False
 * /camera/camera/enable_publish_extrinsic: False
 * /camera/camera/flip_color: False
 * /camera/camera/flip_depth: False
 * /camera/camera/flip_ir: False
 * /camera/camera/ir_format: Y10
 * /camera/camera/ir_fps: 30
 * /camera/camera/ir_height: 480
 * /camera/camera/ir_info_uri: 
 * /camera/camera/ir_width: 640
 * /camera/camera/oni_log_level: verbose
 * /camera/camera/oni_log_to_console: False
 * /camera/camera/oni_log_to_file: False
 * /camera/camera/product_id: 0
 * /camera/camera/publish_tf: True
 * /camera/camera/serial_number: 
 * /camera/camera/tf_publish_rate: 10.0
 * /camera/camera/use_uvc_camera: True
 * /camera/camera/uvc_camera_format: mjpeg
 * /camera/camera/uvc_flip: False
 * /camera/camera/uvc_product_id: 0x0501
 * /camera/camera/uvc_retry_count: 100
 * /camera/camera/uvc_vendor_id: 0x2bc5
 * /camera/camera/vendor_id: 0
 * /rosdistro: noetic
 * /rosversion: 1.17.0

NODES
  /camera/
    camera (astra_camera/astra_camera_node)

auto-starting new master
process[master]: started with pid [36576]
ROS_MASTER_URI=http://localhost:11311

setting /run_id to dd2397f8-da75-11ef-8a09-3c6d66115ee2
process[rosout-1]: started with pid [36593]
started core service [/rosout]
process[camera/camera-2]: started with pid [36600]
[ INFO] [1737738523.737103150]: Starting camera node...
[ INFO] [1737738523.748559925]: Creating camera node...
[ INFO] [1737738523.749587733]: OBCameraNodeFactory::OBCameraNodeFactory
[ INFO] [1737738523.749652119]: Initializing OBCameraNodeFactory...
[ INFO] [1737738523.769552807]: init Done
[ INFO] [1737738523.769633449]: Query device
[ INFO] [1737738523.769708972]: Creating camera node done...
[ INFO] [1737738523.769784910]: Found 1 devices
[ INFO] [1737738523.870010002]: Device connected: (name, Astra) (uri, 2bc5/0403@1/17) (vendor, Orbbec)
[ INFO] [1737738523.870089429]: try lock multi process lock
[ INFO] [1737738523.870116469]: Trying to open device: 2bc5/0403@1/17
[ INFO] [1737738523.970282231]: OBCameraNodeFactory::onDeviceConnected Open device start
[ INFO] [1737738524.052419397]: OBCameraNodeFactory::onDeviceConnected Open device done, STATUS 0
[ INFO] [1737738524.052530856]: Device connected: Astra serial number: 18042020299
[ INFO] [1737738524.052557897]: Start device 
[ INFO] [1737738524.122198639]: set depth video mode Resolution :640x480@30Hz
format PIXEL_FORMAT_DEPTH_1_MM
[ INFO] [1737738524.122628220]: set ir video mode Resolution :640x480@30Hz
format PIXEL_FORMAT_GRAY8
[ INFO] [1737738524.125560312]: OBCameraNode::setupUVCCamera
[ WARN] [1737738524.125666107]: Publishing dynamic camera transforms (/tf) at 10 Hz
[ INFO] [1737738524.146595595]: open uvc camera
[ INFO] [1737738524.226468977]: uvc config: vendor_id: 0
product_id: 0
width: 640
height: 480
fps: 30
serial_number: 18042020299
format: mjpeg

unsupported descriptor subtype VS_COLORFORMAT
unsupported descriptor subtype VS_COLORFORMAT
[ INFO] [1737738524.482450181]: open camera success
[ INFO] [1737738524.482571689]: enable color auto exposure
[ INFO] [1737738524.485541862]: backlight compensation range [0, 65535]
[ INFO] [1737738524.485580071]: set color backlight compensation to 0
[ INFO] [1737738524.490637413]: using default calibration URL
[ INFO] [1737738524.490699143]: camera calibration URL: file:///home/jetson/.ros/camera_info/rgb_camera.yaml
[ INFO] [1737738524.490810123]: Unable to open camera calibration file [/home/jetson/.ros/camera_info/rgb_camera.yaml]
[ WARN] [1737738524.490844716]: Camera calibration file /home/jetson/.ros/camera_info/rgb_camera.yaml not found.
[ INFO] [1737738524.521916217]: using default calibration URL
[ INFO] [1737738524.521999580]: camera calibration URL: file:///home/jetson/.ros/camera_info/ir_camera.yaml
[ INFO] [1737738524.522062078]: Unable to open camera calibration file [/home/jetson/.ros/camera_info/ir_camera.yaml]
[ WARN] [1737738524.522090847]: Camera calibration file /home/jetson/.ros/camera_info/ir_camera.yaml not found.
[ INFO] [1737738524.522178593]: OBCameraNode initialized
[ INFO] [1737738524.522207810]: Start device  done



$ rostopic echo /camera/depth/camera_info
header: 
  seq: 0
  stamp: 
    secs: 1737739603
    nsecs: 839713729
  frame_id: "camera_depth_optical_frame"
height: 480
width: 640
distortion_model: "plumb_bob"
D: [0.0, 0.0, 0.0, 0.0, 0.0]
K: [576.8049926757812, 0.0, 318.2950134277344, 0.0, 576.8049926757812, 254.51499938964844, 0.0, 0.0, 1.0]
R: [0.9999359846115112, -0.011242999695241451, -0.001120670000091195, 0.011238999664783478, 0.9999300241470337, -0.0035657500848174095, 0.0011606799671426415, 0.00355292996391654, 0.9999930262565613]
P: [576.8049926757812, 0.0, 318.2950134277344, 0.0, 0.0, 576.8049926757812, 254.51499938964844, 0.0, 0.0, 0.0, 1.0, 0.0]
binning_x: 0
binning_y: 0
roi: 
  x_offset: 0
  y_offset: 0
  height: 0
  width: 0
  do_rectify: False
---


$ rostopic echo /camera/color/camera_info
header: 
  seq: 0
  stamp: 
    secs: 1737739603
    nsecs: 803281458
  frame_id: "camera_color_optical_frame"
height: 480
width: 640
distortion_model: "plumb_bob"
D: [0.0, 0.0, 0.0, 0.0, 0.0]
K: [602.9929809570312, 0.0, 325.0480041503906, 0.0, 602.9929809570312, 238.58999633789062, 0.0, 0.0, 1.0]
R: [0.9999359846115112, -0.011242999695241451, -0.001120670000091195, 0.011238999664783478, 0.9999300241470337, -0.0035657500848174095, 0.0011606799671426415, 0.00355292996391654, 0.9999930262565613]
P: [602.9929809570312, 0.0, 325.0480041503906, 0.0, 0.0, 602.9929809570312, 238.58999633789062, 0.0, 0.0, 0.0, 1.0, 0.0]
binning_x: 0
binning_y: 0
roi: 
  x_offset: 0
  y_offset: 0
  height: 0
  width: 0
  do_rectify: False
---


_____________________________________________________________________________
## INSTALL ROS Wrapper HLS(Hitachi-LG Sensor) LFCD LDS(Laser Distance Sensor)
_____________________________________________________________________________
## https://emanual.robotis.com/docs/en/platform/turtlebot3/appendix_lds_01/


## INSTALL

sudo apt-get install ros-noetic-hls-lfcd-lds-driver

## Set Permission for LDS-01
$ sudo chmod a+rw /dev/ttyUSB0



roscore
source /home/jetson/ros_ws/devel/setup.bash


## Run hlds_laser_publisher Node
roslaunch hls_lfcd_lds_driver hlds_laser.launch
WORK

# OR THIS
## Run hlds_laser_publisher Node with RViz
roslaunch hls_lfcd_lds_driver view_hlds_laser.launch
WORK

SOMETIMES NOT WORK THROUGH USB HUB... just reconnect usb hub


---

## RUN Gmapping node



rosrun gmapping slam_gmapping scan:=/scan map:=/gmapping/map




_________________________________________________
# INSTALL ROS Wrapper for Intel® RealSense™ Devices
_________________________________________________
## https://github.com/IntelRealSense/realsense-ros
## https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy

## These are packages for using Intel RealSense cameras (D400 series SR300 camera and T265 Tracking Module) with ROS.

sudo apt-get install ros-$ROS_DISTRO-realsense2-camera
sudo apt-get install ros-noetic-realsense-ros




roslaunch realsense2_camera rs_t265.launch
#WORK

To visualize the pose output and frames in RViz, start:
roslaunch realsense2_camera demo_t265.launch
# WORK

## rosrun image_view image_view image:=/camera/fisheye1/image_raw
## NOT WORK



### FIX SETTING IN FILE 
$ sudo nano '/opt/ros/noetic/share/realsense2_camera/launch/rs_t265.launch' 

<!--
Important Notice: For wheeled robots, odometer input is a requirement for robust
and accurate tracking. The relevant APIs will be added to librealsense and
ROS/realsense in upcoming releases. Currently, the API is available in the
https://github.com/IntelRealSense/librealsense/blob/master/third-party/libtm/libtm/include/TrackingDevice.h#L508-L515.
-->
<launch>
  <arg name="serial_no"           default=""/>
  <arg name="usb_port_id"         default=""/>
  <arg name="device_type"         default="t265"/>
  <arg name="json_file_path"      default=""/>
  <arg name="camera"              default="camera"/>
  <arg name="tf_prefix"           default="$(arg camera)"/>

  <arg name="fisheye_width"       default="848"/> 
  <arg name="fisheye_height"      default="800"/>
  <arg name="enable_fisheye1"     default="true"/>
  <arg name="enable_fisheye2"     default="true"/>

  <arg name="fisheye_fps"         default="30"/>

$ rosrun image_view image_view image:=/camera/fisheye1/image_raw
## WORK!!


$ rostopic list
/camera/accel/imu_info
/camera/accel/metadata
/camera/accel/sample
/camera/fisheye1/camera_info
/camera/fisheye1/image_raw
/camera/fisheye1/image_raw/compressed
/camera/fisheye1/image_raw/compressed/parameter_descriptions
/camera/fisheye1/image_raw/compressed/parameter_updates
/camera/fisheye1/image_raw/compressedDepth
/camera/fisheye1/image_raw/compressedDepth/parameter_descriptions
/camera/fisheye1/image_raw/compressedDepth/parameter_updates
/camera/fisheye1/image_raw/theora
/camera/fisheye1/image_raw/theora/parameter_descriptions
/camera/fisheye1/image_raw/theora/parameter_updates
/camera/fisheye1/metadata
/camera/fisheye2/camera_info
/camera/fisheye2/image_raw
/camera/fisheye2/image_raw/compressed
/camera/fisheye2/image_raw/compressed/parameter_descriptions
/camera/fisheye2/image_raw/compressed/parameter_updates
/camera/fisheye2/image_raw/compressedDepth
/camera/fisheye2/image_raw/compressedDepth/parameter_descriptions
/camera/fisheye2/image_raw/compressedDepth/parameter_updates
/camera/fisheye2/image_raw/theora
/camera/fisheye2/image_raw/theora/parameter_descriptions
/camera/fisheye2/image_raw/theora/parameter_updates
/camera/fisheye2/metadata
/camera/gyro/imu_info
/camera/gyro/metadata
/camera/gyro/sample
/camera/odom/metadata
/camera/odom/sample
/camera/realsense2_camera_manager/bond
/camera/tracking_module/parameter_descriptions
/camera/tracking_module/parameter_updates
/diagnostics
/rosout
/rosout_agg
/tf
/tf_static




This will stream all camera sensors and publish on the appropriate ROS topics.

The T265 sets its usb unique ID during initialization and without this parameter it wont be found. 
Once running it will publish, among others, the following topics:


# FROM SITE

/camera/odom/sample
/camera/accel/sample
/camera/gyro/sample

/camera/fisheye1/image_raw
/camera/fisheye2/image_raw


# IN REAL
$ rostopic list
/camera/odom/sample
/camera/accel/sample
/camera/gyro/sample

/camera/accel/imu_info
/camera/accel/metadata

/camera/gyro/imu_info
/camera/gyro/metadata

/camera/odom/metadata

/camera/realsense2_camera_manager/bond
/camera/tracking_module/parameter_descriptions
/camera/tracking_module/parameter_updates
/diagnostics
/rosout
/rosout_agg
/tf
/tf_static

roslaunch realsense2_camera rs_t265.launch
....
 * /camera/realsense2_camera/depth_fps: 30
 * /camera/realsense2_camera/depth_frame_id: camera_depth_frame
 * /camera/realsense2_camera/depth_height: 480
 * /camera/realsense2_camera/depth_optical_frame_id: camera_depth_opti...
 * /camera/realsense2_camera/depth_width: 640
 * /camera/realsense2_camera/device_type: t265
 * /camera/realsense2_camera/enable_accel: True
 * /camera/realsense2_camera/enable_color: True
 * /camera/realsense2_camera/enable_confidence: True
 * /camera/realsense2_camera/enable_depth: True

 * /camera/realsense2_camera/enable_fisheye1: False
 * /camera/realsense2_camera/enable_fisheye2: False
 * /camera/realsense2_camera/enable_fisheye: False

 * /camera/realsense2_camera/enable_gyro: True
 * /camera/realsense2_camera/enable_infra1: False
 * /camera/realsense2_camera/enable_infra2: False

...
rocess[camera/realsense2_camera_manager-1]: started with pid [171585]
process[camera/realsense2_camera-2]: started with pid [171586]
[ INFO] [1737320390.394005499]: Initializing nodelet with 8 worker threads.
[ INFO] [1737320390.548165268]: RealSense ROS v2.3.2
[ INFO] [1737320390.548250743]: Built with LibRealSense v2.50.0
[ INFO] [1737320390.548287993]: Running with LibRealSense v2.50.0
[ INFO] [1737320390.588552907]:  
[ INFO] [1737320390.610509603]: Device with serial number 905312111138 was found.

[ INFO] [1737320390.613709521]: Device with physical ID 2-1.1-6 was found.
[ INFO] [1737320390.613879575]: Device with name Intel RealSense T265 was found.
[ INFO] [1737320390.614640178]: Device with port number 2-1.1 was found.
[ INFO] [1737320390.614959677]: Device USB type: 3.1
[ INFO] [1737320390.622279834]: No calib_odom_file. No input odometry accepted.
[ INFO] [1737320390.622821261]: getParameters...
[ INFO] [1737320390.731376283]: setupDevice...
[ INFO] [1737320390.731472126]: JSON file is not provided
[ INFO] [1737320390.731495295]: ROS Node Namespace: camera
[ INFO] [1737320390.731518528]: Device Name: Intel RealSense T265
[ INFO] [1737320390.731542593]: Device Serial No: 905312111138
[ INFO] [1737320390.731559041]: Device physical port: 2-1.1-6
[ INFO] [1737320390.731574274]: Device FW version: 0.2.0.951
[ INFO] [1737320390.731588802]: Device Product ID: 0x0B37
[ INFO] [1737320390.731604227]: Enable PointCloud: Off
[ INFO] [1737320390.731618211]: Align Depth: Off
[ INFO] [1737320390.731632356]: Sync Mode: Off
[ INFO] [1737320390.731669509]: Device Sensors: 
[ INFO] [1737320390.731835755]: Tracking Module was found.
[ INFO] [1737320390.731931214]: (Depth, 0) sensor isn't supported by current device! -- Skipping...
[ INFO] [1737320390.732052786]: (Color, 0) sensor isn't supported by current device! -- Skipping...
[ INFO] [1737320390.732091028]: (Confidence, 0) sensor isn't supported by current device! -- Skipping...
[ INFO] [1737320390.732155094]: num_filters: 0
[ INFO] [1737320390.732193207]: Setting Dynamic reconfig parameters.
[ WARN] [1737320390.735813653]: Param '/camera/tracking_module/frames_queue_size' has value 256 that is not in range [0, 32]. Removing this parameter from dynamic reconfigure options.

[ INFO] [1737320390.745962100]: Done Setting Dynamic reconfig parameters.
[ INFO] [1737320390.746147482]: gyro stream is enabled - fps: 200
[ INFO] [1737320390.746185596]: accel stream is enabled - fps: 62
[ INFO] [1737320390.746212125]: pose stream is enabled - fps: 200
[ INFO] [1737320390.746236190]: setupPublishers...
[ INFO] [1737320390.753952297]: setupStreams...
[ INFO] [1737320390.789446325]: SELECTED BASE:Pose, 0
[ INFO] [1737320390.792695942]: RealSense Node Is Up!



__________________________________________________________________________________________________
# INSTALL orb_slam3_ros
__________________________________________________________________________________________________
# https://github.com/thien94/orb_slam3_ros

## INSTALL:

~/ros_ws/src
git clone https://github.com/thien94/orb_slam3_ros.git

# add package "vision_opencv" to src dir

cd ..
catkin build
Finished  <<< orb_slam3_ros                 [ 5 minutes and 11.1 seconds ]

source /home/jetson/ros_ws/devel/setup.bash


## RUN ORB_SLAM3 ROS1 WITH T265

source /home/jetson/ros_ws/devel/setup.bash


# Live stereo-inertial mode with Realsense T265
Modify the original rs_t265.launch to enable fisheye images and imu data (change unite_imu_method to linear_interpolation).

Run rs-enumerate-devices -c to get the calibration parameters and modify config/Stereo-Inertial/RealSense_T265.yaml accordingly. A detailed explaination can be found here.


## RUN ORB_SLAM3 WRAPPER WITH T265

roscore ??? not need...

realsense-viewer

T1: 
roslaunch realsense2_camera rs_t265.launch

T2:
roslaunch orb_slam3_ros live_t265.launch

# ERROR:
[orb_slam3-1] process has died [pid 139213, exit code 255, 
cmd /home/jetson/ros_ws/devel/lib/orb_slam3_ros/ros_mono /camera/image_raw:=/camera/fisheye1/image_raw /imu:=/camera/imu __name:=orb_slam3

# FIX: change info in "live_t265.launch" file 

<launch>
    <param name="use_sim_time" value="false" />
    <!-- ORB-SLAM3 -->
    <node name="orb_slam3" pkg="orb_slam3_ros" type="ros_mono" output="screen">
        <!-- From realsense2_camera node -->
        <remap from="/camera/image_raw"         to="/camera/fisheye1/image_raw"/>
        <remap from="/imu"                      to="/camera/imu"/>

        <!-- Parameters for original ORB-SLAM3 -->
        <param name="voc_file"      type="string" value="$(find orb_slam3_ros)/orb_slam3/Vocabulary/ORBvoc.txt.bin"/>
        <param name="settings_file" type="string" value="$(find orb_slam3_ros)/config/Monocular/RealSense_T265.yaml" />

        <!-- Parameters for ROS -->
        <param name="world_frame_id"    type="string"   value="world" />
        <param name="cam_frame_id"      type="string"   value="camera" />
        <param name="imu_frame_id"      type="string"   value="imu" />
        <param name="enable_pangolin"   type="bool"     value="true" />
    </node>

    <!-- Visualization -->
    <node name="rviz" pkg="rviz" type="rviz" args="-d $(find orb_slam3_ros)/config/orb_slam3_with_imu.rviz" output="screen" />
    
    <!-- Trajectory path -->
    <node pkg="hector_trajectory_server" type="hector_trajectory_server" name="trajectory_server_orb_slam3" output="screen" ns="orb_slam3_ros" >
        <param name="/target_frame_name" value="/world" />
        <param name="/source_frame_name" value="/imu" />
        <param name="/trajectory_update_rate" value="20.0" />
        <param name="/trajectory_publish_rate" value="20.0" />
    </node>
</launch>

roslaunch orb_slam3_ros live_t265.launch
# WORK



roslaunch orb_slam3_ros rs_t265_stereo_inertial.launch

## WITHOUT THIS CHANGE no picture... black screen and waiting for images...
Live stereo-inertial mode with Realsense T265
Modify the original "rs_t265.launch" to enable fisheye images and imu data 
(change unite_imu_method to linear_interpolation).

## WORK WITH big ERRORS in trajectory



roslaunch orb_slam3_ros rs_t265_stereo.launch


## WORK but need test...



$ rostopic list
/camera/accel/imu_info
/camera/accel/metadata
/camera/accel/sample
/camera/fisheye1/camera_info
/camera/fisheye1/image_raw
/camera/fisheye1/image_raw/compressed
/camera/fisheye1/image_raw/compressed/parameter_descriptions
/camera/fisheye1/image_raw/compressed/parameter_updates
/camera/fisheye1/image_raw/compressedDepth
/camera/fisheye1/image_raw/compressedDepth/parameter_descriptions
/camera/fisheye1/image_raw/compressedDepth/parameter_updates
/camera/fisheye1/image_raw/theora
/camera/fisheye1/image_raw/theora/parameter_descriptions
/camera/fisheye1/image_raw/theora/parameter_updates
/camera/fisheye1/metadata
/camera/fisheye2/camera_info
/camera/fisheye2/image_raw
/camera/fisheye2/image_raw/compressed
/camera/fisheye2/image_raw/compressed/parameter_descriptions
/camera/fisheye2/image_raw/compressed/parameter_updates
/camera/fisheye2/image_raw/compressedDepth
/camera/fisheye2/image_raw/compressedDepth/parameter_descriptions
/camera/fisheye2/image_raw/compressedDepth/parameter_updates
/camera/fisheye2/image_raw/theora
/camera/fisheye2/image_raw/theora/parameter_descriptions
/camera/fisheye2/image_raw/theora/parameter_updates
/camera/fisheye2/metadata
/camera/gyro/imu_info
/camera/gyro/metadata
/camera/gyro/sample
/camera/odom/metadata
/camera/odom/sample
/camera/realsense2_camera_manager/bond
/camera/tracking_module/parameter_descriptions
/camera/tracking_module/parameter_updates

/clicked_point
/diagnostics
/initialpose

/move_base_simple/goal
/orb_slam3/all_points
/orb_slam3/camera_pose
/orb_slam3/kf_markers
/orb_slam3/kf_markers_array
/orb_slam3/tracked_key_points
/orb_slam3/tracked_points
/orb_slam3/tracking_image
/orb_slam3/tracking_image/compressed
/orb_slam3/tracking_image/compressed/parameter_descriptions
/orb_slam3/tracking_image/compressed/parameter_updates
/orb_slam3/tracking_image/compressedDepth
/orb_slam3/tracking_image/compressedDepth/parameter_descriptions
/orb_slam3/tracking_image/compressedDepth/parameter_updates
/orb_slam3/tracking_image/theora
/orb_slam3/tracking_image/theora/parameter_descriptions
/orb_slam3/tracking_image/theora/parameter_updates
/orb_slam3_ros/trajectory
/rosout
/rosout_agg
/tf
/tf_static

---

## >>>>>>> LAUNCH ERROR <<<<<<<<
$ roslaunch orb_slam3_ros live_t265.launch
RLException: [live_t265.launch] is neither a launch file in package [orb_slam3_ros] nor is [orb_slam3_ros] a launch file name
## FIX: delete the file CATKIN_IGNORE from /src dir ##

## REBUILD:
catkin build

## WORK:
source /home/jetson/ros_ws/devel/setup.bash
roscd orb_slam3_ros
roslaunch orb_slam3_ros live_t265.launch

... logging to /home/jetson/.ros/log/296e2ffc-e605-11ef-aa1c-3c6d66115ee2/roslaunch-ubuntu-468485.log
Checking log directory for disk usage. This may take a while.
Press Ctrl-C to interrupt
Done checking log file disk usage. Usage is <1GB.
...

$ rostopic echo /orb_slam3/camera_pose
header: 
  seq: 16718
  stamp: 
    secs: 1739016066
    nsecs: 870544434
  frame_id: "world"
pose: 
  position: 
    x: 0.003402560483664274
    y: -0.0006122348713688552
    z: -0.006186985410749912
  orientation: 
    x: -0.00014934131468180567
    y: 0.008978662081062794 	<<<<<<<------------
    z: -0.0012419618433341384
    w: 0.9999589323997498
---


$ rostopic echo /camera/odom/sample
header: 
  seq: 160
  stamp: 
    secs: 1739016291
    nsecs: 744120598
  frame_id: "camera_odom_frame"
child_frame_id: "camera_pose_frame"
pose: 
  pose: 
    position: 
      x: -0.057263825088739395
      y: -0.030677953734993935
      z: 0.003937949426472187
    orientation: 
      x: -0.023132214322686195
      y: -0.046975016593933105
      z: -0.009999110363423824
      w: 0.9985781311988831
  covariance: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001]
twist: 
  twist: 
    linear: 
      x: 0.0015892559617823715
      y: -0.0021415687074940997
      z: 0.0026848243285079744
    angular: 
      x: -0.0014736614367798126
      y: -0.0031631411088150797
      z: 0.002346181302699396
  covariance: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001]
---


# Which topic should I subscribe to if I want to output real-time location information of the entire system? #13

# Hi @Shang-cl , the /orb_slam3/camera_pose and /orb_slam3/body_odom are the two topics that you can subscribe to for the real-time pose data.


# Is the pose information corresponding to these two topics the camera and the IMU? Which one should be used in practical engineering applications?

# Yes, that is correct. The topic you should use depends on your requirements, but generally the IMU-based body odom is preferred. Do note that because of the way ORB-SLAM3 works, the output rate of the body odometry is the same or lower than the camera framerate (20-30Hz), and not the same as the IMU frequency (200Hz).

For now, the easiest solution is to move the camera randomly in all directions at the start of each experiment, similar to the motion in this video https://youtu.be/O04EywptgH8

'pure direction Pre-motion is one of the degenerate cases of the visual odometry problem'





__________________________________________________________________________________________________
# INSTALL orb_slam3_ros_wrapper
__________________________________________________________________________________________________
## https://github.com/thien94/orb_slam3_ros_wrapper


1. ORB-SLAM3

## Clone
cd ~
git clone https://github.com/thien94/ORB_SLAM3.git ORB_SLAM3

# Work with Ubuntu 20.04, no additional installation of OpenCV or C++ required:

# Update CMakeLists.txt to use OpenCV 4.2 mimimum.
# Update CMakeLists.txt to use C++14 instead of C++11.


## Build:
cd ORB_SLAM3
chmod +x build.sh
./build.sh

# Make sure that libORB_SLAM3.so is created in the ORB_SLAM3/lib folder. 
# If not, check the issue list from the original repo and retry.


2. Install orb_slam3_ros_wrapper

Clone the package. Note that it should be a catkin build workspace.

cd ~/catkin_ws/src/
git clone https://github.com/thien94/orb_slam3_ros_wrapper.git
# Open CMakeLists.txt and change the directory that leads to ORB-SLAM3 library at the beginning of the file (default is home folder ~/)
cd ~/catkin_ws/src/orb_slam3_ros_wrapper/
nano CMakeLists.txt

# Change this to your installation of ORB-SLAM3. Default is ~/
set(ORB_SLAM3_DIR
   $ENV{HOME}/ORB_SLAM3
)

# Build the package normally.
cd ~/catkin_ws/
catkin build

Next, copy the ORBvoc.txt file from ORB-SLAM3/Vocabulary/ folder to the config folder in this package. Alternatively, you can change the voc_file param in the launch file to point to the right location.

(Optional) Install hector-trajectory-server to visualize the trajectory.

sudo apt install ros-[DISTRO]-hector-trajectory-server
If everything works fine, you can now try the different launch files in the launch folder.


3. How to run

EuRoC dataset:
In one terminal, launch the node:
roslaunch orb_slam3_ros_wrapper euroc_monoimu.launch
In another terminal, playback the bag:
rosbag play MH_01_easy.bag
Similarly for other sensor types.

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
## RUN ORB_SLAM3 WRAPPER WITH uSB_CAM
T1:
roscore

T2:
roslaunch usb_cam usb_cam-test.launch

T3:
source /home/jetson/ros_ws/devel/setup.bash
roslaunch orb_slam3_ros_wrapper usb_cam.launch 


FILE "usb_cam.launch"
<launch>
    <!-- ORB-SLAM3 -->
    <node name="orb_slam3_mono" pkg="orb_slam3_ros_wrapper" type="orb_slam3_ros_wrapper_mono" output="screen">
        <!-- From realsense2_camera node -->
        <remap from="/camera/image_raw"         to="/usb_cam/image_raw"/>
        #<remap from="/imu"                      to="/camera/imu"/>

        <!-- Parameters for original ORB-SLAM3 -->
        <param name="voc_file"      type="string"   value="$(find orb_slam3_ros_wrapper)/config/ORBvoc.txt" />
        <param name="settings_file" type="string"   value="$(find orb_slam3_ros_wrapper)/config/EuRoC.yaml" />

        <!-- Parameters for ROS -->
        <param name="world_frame_id"    type="string"   value="world" />
        <param name="cam_frame_id"      type="string"   value="camera" />
        <param name="enable_pangolin"   type="bool"     value="true" />
    </node>

    <!-- Visualization - RViz-->
    <node name="rviz" pkg="rviz" type="rviz" args="-d $(find orb_slam3_ros_wrapper)/config/orb_slam3_with_imu.rviz" output="screen" />

    <node pkg="hector_trajectory_server" type="hector_trajectory_server" name="trajectory_server_orb_slam3" output="screen" ns="orb_slam3_ros" >
        <param name="/target_frame_name" value="/world" />
        <param name="/source_frame_name" value="/camera" />
        <param name="/trajectory_update_rate" value="20.0" />
        <param name="/trajectory_publish_rate" value="20.0" />
    </node>
</launch>



>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

## RUN ORB_SLAM3 WRAPPER WITH T265
T1:
source /home/jetson/ast_ws/devel/setup.bash
roscore

T2:
# test t265 
$ realsense-viewer

source /home/jetson/ros_ws/devel/setup.bash
$ roslaunch realsense2_camera rs_t265.launch
## WORK!

T3:
source /home/jetson/ast_ws/devel/setup.bash
# roslaunch orb_slam3_ros_wrapper live_t265.launch

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.
$ rostopic list

/cam0/image_raw
/cam0/image_raw/mouse_click
...
/clicked_point
/diagnostics

/initialpose

/move_base_simple/goal
empty

/orb_slam3/camera_pose
#rostopic type: geometry_msgs/PoseStamped

header: 
  seq: 8005
  stamp: 
    secs: 1738330838
    nsecs: 940361977
  frame_id: "world"
pose: 
  position: 
    x: -0.004764787387102842
    y: 3.8100144593045115e-05
    z: 0.001778802485205233
  orientation: 
    x: 0.0010951267322525382
    y: -0.01550188846886158
    z: 0.002721738535910845
    w: 0.999875545501709
---

/orb_slam3/map_points	-->> rviz PointCloud2
#rostopic type: sensor_msgs/PointCloud2

 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 222, 65, 189, 154, 134, 37, 188, 122, 2, 149, 60, 219, 41, 55, 189, 3, 49, 158, 186, 196, 79, 65, 60, 125, 136, 31, 190, 1

/orb_slam3_ros/trajectory -->> rviz Path
#rostopic type: nav_msgs/Path

  - 
    header: 
      seq: 0
      stamp: 
        secs: 1738330797
        nsecs: 541690588
      frame_id: "/world"
    pose: 
      position: 
        x: -0.008545499294996262
        y: -6.865383329568431e-05
        z: 0.0010968251153826714
      orientation: 
        x: 0.004959535003033313
        y: -0.07131768999305499
        z: 0.012589417396376139
        w: 0.9973618684691269
---

/orb_slam3_ros/syscommand


/rosout
/rosout_agg
/tf
/tf_static




## RVIZ2 ODOMETRY ERROR:
[INFO] [1738334506.449408107] [rviz]: Message Filter dropping message: frame 'camera_odom_frame' at time 1738334505.995 for reason 'discarding message because the queue is full'





------------------------------------------------------
# PROBLEM ROS ORB_SLAM3 Init pose is not align to the World
------------------------------------------------------
## https://answers.ros.org/question/358514/

Can you please share your TF tree:
$ rosrun rqt_tf_tree rqt_tf_tree
Check the transform between map and t265_pose_frame


The problem was temporarily solved by setting the "static_map" option to "false".
I believe the main problem comes from the map frame convention published by the mapping package. 
Anyway this temporary solved my issue.

------------------------------------------------------
# Problem Setting An Initial Pose In ORB_SLAM2
------------------------------------------------------
# https://answers.ros.org/question/363905/

I am running ORB SLAM 2 on a drone with a downward facing, RGBD camera. I'm using Gazebo to simulate and Rviz to see the visionpose ROS topic. I am having an issue with the visionpose being incorrect because the pose is initialised as if the camera is pointing straight forward towards the horizon by default instead of straight down.

If anyone is familiar with the ORBSLAM2 code, to try and initialise the pose to suit my initial conditions, in the Tracking::StereoInitialization() function, I have replaced the line: `mCurrentFrame.SetPose(cv::Mat::eye(4,4,CV32F)); With: mCurrentFrame.SetPose(cv::Mat(4,4,CV32F,quatrot));wherequat_rot` is a quaternion that represents a 90 degree pitch down on the Z axis.

I used an online calculator to find that the quaternion for the pitch down would be: (0.7071, 0, 0, -0.7071) or more simply, (0.7071 - 0.7071k).

I declared the quat_rot quaternion with those numbers: float quat_rot[16] =
{0.7071, 0, 0, -0.7071,
0, 0.7071, -0.7071, 0,
0, -0.7071, 0.7071, 0,
-0.7071, 0, 0, 0.7071};

But when using Rviz to check the pose, it is only rotated down at 45 degrees. I have tried swapping the position of the scalar number in the quaternion after reading about the differences in how Eigen processes quaternions but it doesn't seem like Eigen is used for this particular part. I have also tried with some different numbers but none give a rotation past 45 degrees without throwing errors.

I'm out of ideas and would greatly appreciate any input, perhaps the basis of the question is wrong and I should be doing this elsewhere in the code, I don't know, maybe someone can point me in the right direction.

Thanks!

Asked by EuanM on 2020-10-21 08:24:24 UTC







------------------------------------------------------

Hi, if you still have issue with this, you may define a parameter in the launch file as:
<param name="enable_pangolin" type="bool" value="false"/>

This should deactivate the viewer (Pangolin only)


$ rostopic list

/camera/accel/imu_info
/camera/accel/metadata
/camera/accel/sample
/camera/fisheye1/camera_info
/camera/fisheye1/image_raw
/camera/fisheye1/image_raw/compressed
/camera/fisheye1/image_raw/compressed/parameter_descriptions
/camera/fisheye1/image_raw/compressed/parameter_updates
/camera/fisheye1/image_raw/compressedDepth
/camera/fisheye1/image_raw/compressedDepth/parameter_descriptions
/camera/fisheye1/image_raw/compressedDepth/parameter_updates
/camera/fisheye1/image_raw/theora
/camera/fisheye1/image_raw/theora/parameter_descriptions
/camera/fisheye1/image_raw/theora/parameter_updates
/camera/fisheye1/metadata
/camera/fisheye2/camera_info
/camera/fisheye2/image_raw
/camera/fisheye2/image_raw/compressed
/camera/fisheye2/image_raw/compressed/parameter_descriptions
/camera/fisheye2/image_raw/compressed/parameter_updates
/camera/fisheye2/image_raw/compressedDepth
/camera/fisheye2/image_raw/compressedDepth/parameter_descriptions
/camera/fisheye2/image_raw/compressedDepth/parameter_updates
/camera/fisheye2/image_raw/theora
/camera/fisheye2/image_raw/theora/parameter_descriptions
/camera/fisheye2/image_raw/theora/parameter_updates
/camera/fisheye2/metadata
/camera/gyro/imu_info
/camera/gyro/metadata
/camera/gyro/sample
/camera/odom/metadata
/camera/odom/sample
/camera/realsense2_camera_manager/bond
/camera/tracking_module/parameter_descriptions
/camera/tracking_module/parameter_updates
/diagnostics
/rosout
/rosout_agg
/tf
/tf_static



>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


$ roslaunch orb_slam3_ros_wrapper live_t265.launch
## ERROR 

[orb_slam3_mono-1] process has died [pid 23447, exit code -11, 

cmd /home/jetson/ros_ws/devel/lib/orb_slam3_ros_wrapper/orb_slam3_ros_wrapper_mono /camera/image_raw:=/camera/fisheye1/image_raw /imu:=/camera/imu __name:=orb_slam3_mono 

__log:=/home/jetson/.ros/log/57db4dba-da49-11ef-9763-3c6d66115ee2/orb_slam3_mono-1.log].



## roslaunch orb_slam3_ros_wrapper euroc_monoimu.launch

it seems I have missed getting EuRoc Dataset:
wget -c http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/machine_hall/MH_01_easy/MH_01_easy.zip

then Unzip to MH01 folder :)

## rosbag play MH_01_easy.bag



## roslaunch orb_slam3_ros_wrapper TUM1.yaml

roslaunch orb_slam3_ros_wrapper live_t265.launch
roslaunch orb_slam3_ros_wrapper live_t265.launch camera:=camera1 


# rosrun image_transport republish raw in:=/usb_cam/image_raw raw out:=/camera/image_raw

rosrun image_transport republish raw in:=/camera/image_raw raw out:=/orb_slam3_mono/image_raw
/orb_slam3_mono

roslaunch orb_slam3_ros_wrapper euroc_mono.launch




 ME:

[orb_slam3_mono-1] process has died [pid 119550, exit code -11, 
cmd /home/jetson/ros_ws/devel/lib/orb_slam3_ros_wrapper/orb_slam3_ros_wrapper_mono /camera/image_raw:=/camera/fisheye1/image_raw /cam0/image_raw:=/camera/fisheye1/image_raw /imu:=/camera/imu __name:=orb_slam3_mono

__log:=/home/jetson/.ros/log/059a90ee-d69b-11ef-a6ad-3c6d66115ee2/orb_slam3_mono-1.log].

[orb_slam3_mono-1] process has died [pid 134407, exit code -11, 
cmd /home/jetson/ros_ws/devel/lib/orb_slam3_ros_wrapper/orb_slam3_ros_wrapper_mono /camera/image_raw:=/usb_cam/image_raw /imu:=/camera/imu __name:=orb_slam3_mono 
__log:=/home/jetson/.ros/log/c0ee6cd2-d69e-11ef-b1fe-3c6d66115ee2/orb_slam3_mono-1.log].


>>>>

SOMEONE:
[orb_slam3_mono-1] process has died [pid 26263, exit code -5, 
cmd /home/xxx/ros1_ws/devel/lib/orb_slam3_ros_wrapper/orb_slam3_ros_wrapper_mono /camera/image_raw:=/usb_cam/image_raw __name:=orb_slam3_mono 

WORKING EXAPMLE...

>>

## https://github.com/thien94/orb_slam3_ros_wrapper/issues/20
I am running ROS1 Noetic on Ubuntu 20.04. I have live camera data coming out of the image_transport republishnode that is being piped into the orb_slam3_ros_wrapper mono node. You can see the Node Graph below showing the image being piped in.

Throwing print statements into the mono node code shows that the subscriber is indeed grabbing camera data. ORB_SLAM3 appears to open up properly. However, I get the sad black screen showing "WAITING FOR IMAGES", as seen below.

Any idea what is preventing ORB_SLAM3 from working, when camera data is clearly being fed into it? Thanks for the help!









<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

## Topics
The following topics are published by each node:

/orb_slam3/map_points (PointCloud2): all keypoints being tracked.

/orb_slam3/camera_pose (PoseStamped): current left camera pose in world frame, as returned by ORB-SLAM3.

tf: transformation from camera frame to world frame.


## Params
Enable / disable pangolin viewer: enable_pangolin

For monocular/stereo case, 
use world_roll, world_pitch, world_yaw in the launch file to rotate the world frame if necessary 
(otherwise the first KF will be the world frame).
The world frame will be rotated by the provided roll-pitch-yaw angles (in rad) in that order.


















_________________________________________________
# INSTALL usb_cam For using a standard USB camera 
_________________________________________________
sudo apt install ros-${ROS_DISTRO}-usb-cam
rosparam set usb_cam/pixel_format yuyv

# FIND ROS POCKET # This command will return the package path if it's installed.
rospack find usb_cam
/opt/ros/noetic/share/usb_cam


## NOT WORK @@@ use this - sudo apt install ros-${ROS_DISTRO}-ros-core
roscd usb_cam


##RUN:

roscore 
rosrun usb_cam usb_cam_node
roslaunch usb_cam usb_cam-test.launch


# Republish the ROS topic to /camera/image_raw.

rosrun image_transport republish raw in:=/usb_cam/image_raw raw out:=/camera/image_raw

rosrun image_view image_view image:=/usb_cam/image_raw



_________________________________________________



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
## https://habr.com/ru/articles/689168/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Установка ORB_SLAM3 ROS noetic

Пакет устанавливается не в catkin_ws!

В целях экономии места и терпения опишем порядок установки тезисно:
---libraries---

sudo apt-get install libboost-all-dev libboost-dev libssl-dev libpython2.7-dev libeigen3-dev


---Pangolin---

cd ~
git clone https://github.com/stevenlovegrove/Pangolin
cd Pangolin
./scripts/install_prerequisites.sh recommended
cmake -B build -GNinja
cmake --build build

---opencv---
*https://qengineering.eu/install-opencv-4.5-on-raspberry-pi-4.html

wget https://github.com/Qengineering/Install-OpenCV-Raspberry-Pi-32-bits/raw/main/OpenCV-4-5-5.sh
sudo chmod 755 ./OpenCV-4-5-5.sh
./OpenCV-4-5-5.sh

---ORB_SLAM3---

git clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git ORB_SLAM3
cd ORB_SLAM3
chmod +x build.sh
sed -i 's/++11/++14/g' CMakeLists.txt
./build.sh



ORB_SLAM установлен, но необходимо еще собрать для него ROS ноды, которые не устанавливаются автоматически при инсталляции (при запуске ./build.sh):


1. Изменим CMakeLists.txt:

cd /home/pi/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3
sed -i 's/++11/++14/g' CMakeLists.txt

здесь же закомментируем AR nodes, т.к. они не собираются:

# Node for monocular camera (Augmented Reality Demo)
#rosbuild_add_executable(MonoAR
#src/AR/ros_mono_ar.cc
#src/AR/ViewerAR.h
#src/AR/ViewerAR.cc
#)
#target_link_libraries(MonoAR
#${LIBS}
#)

здесь же в начало файла CMakeLists.txt добавим:

project(ORB_SLAM3)



2. Изменим build_ros.sh (/home/pi/ORB_SLAM3):

echo "Building ROS nodes"
cd Examples_old/ROS/ORB_SLAM3
mkdir build
cd build
cmake .. -DROS_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=/usr/bin/python3
make -j



3.установим библиотеки:
---Sophus---
cd ~
git clone https://github.com/strasdat/Sophus.git
cd Sophus
mkdir build && cd build && cmake .. && sudo make install
---fmt---
sudo apt install libfmt-dev


Заменим Sophus из /home/pi/ORB_SLAM3/Thirdparty/Sophus
на Sophus /home/pi/Sophus
*Удалить директорию и скопировать ту, что сбилдили выше.


4. Изменить топики, которые будет «слушать» ORB_SLAM3, чтобы камера(raspicam_node) и ORB_SLAM общались в одном топике:

nano /home/pi/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3/src/ros_mono.cc
ros::Subscriber sub = nodeHandler.subscribe("/camera/image", 1, &ImageGrabber::GrabImage,&igb);

*это можно не делать именно здесь, но потом сделать remap в launch файле с камерой. Ниже будет пометка.


5. Собрать ORB_SLAM3 ноды:


#export ROS_PACKAGE_PATH=${ROS_PACKAGE_PATH}:/home/pi/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3
export ROS_PACKAGE_PATH=${ROS_PACKAGE_PATH}:/home/jetson/_packages/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3


6. Теперь, когда все готово, можно запустить все необходимые ноды, совместно с нодами ORB_SLAM3, используя уже 4-е терминала:

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
###### roscore
###### bash -c "source /home/jetson/catkin_ws/devel/setup.bash; roslaunch usb_cam usb_cam-test.launch" 
###### bash -c "source /home/pi/catkin_ws/devel/setup.bash; cd /home/pi/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3; ./Mono /home/pi/ORB_SLAM3/Vocabulary/ORBvoc.txt /home/pi/ORB_SLAM3/Examples/Monocular/TUM1.yaml"

rosrun image_transport republish raw in:=/usb_cam/image_raw raw out:=/camera/image_raw

bash -c "source /opt/ros/noetic/setup.bash; roscore"
## WORK!!

roslaunch usb_cam usb_cam-test.launch
## WEBCAM 0 WORK!!

bash -c "source /opt/ros/noetic/setup.bash; cd /home/jetson/_packages/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3; ./Mono /home/jetson/_packages/ORB_SLAM3/Vocabulary/ORBvoc.txt /home/jetson/_packages/ORB_SLAM3/Examples/Monocular/TUM1.yaml"
## WORK !! !


bash -c "source /opt/ros/noetic/setup.bash; cd /home/jetson/_packages/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3; ./Mono /home/jetson/_packages/ORB_SLAM3/Vocabulary/ORBvoc.txt /home/jetson/_packages/ORB_SLAM3/Examples/Monocular/TUM1.yaml"


$ ./build/mono_tum Vocabulary/ORBvoc.txt Examples/Monocular/TUM1.yaml /home/jetson/_dataset/videos/rgbd_dataset_freiburg1_room



#sudo bash -c "source /home/pi/catkin_ws/devel/setup.bash; roslaunch ros_mpu6050_node mpu6050.launch"
sudo bash -c "source /opt/ros/noetic/setup.bash; roslaunch ros_mpu6050_node mpu6050.launch"

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

*Теперь, если планируется использовать гироскоп/акселерометр, поправим соответствующую ноду.
nano /home/pi/catkin_ws/src/ros_mpu6050_node/launch/mpu6050.launch
Здесь надо выставить частоту — 100-200 Гц, а также сделать remap (перенаправление) топиков, чтобы imu публиковал данные в топик, который будет слушать ORB_SLAM3.

mpu6050.launch
<!-- http://wiki.ros.org/roslaunch/XML/node -->
<launch>
   <node name="mpu6050_node" pkg="ros_mpu6050_node" type="mpu6050_node" output="screen">
      <!--<rosparam file="$(find mypackage)/config/example.yaml" command="load" />-->
      <param name="frequency" type="int" value="200" />
      <param name="frame_id" type="str" value="base_imu" />
      <param name="ax" type="int" value="0" />
      <param name="ay" type="int" value="0" />
      <param name="az" type="int" value="0" />
      <param name="gx" type="int" value="0" />
      <param name="gy" type="int" value="0" />
      <param name="gz" type="int" value="0" />
      <param name="ado" type="bool" value="false" />
      <param name="debug" type="bool" value="false" />
      <remap from="/imu/data" to="/imu"/>
   </node>
</launch>


Остались настройки ORB_SLAM3

Здесь немного посложнее.

Параметры хранятся в файлах формата yaml, поэтому править необходимо их. Что мы и сделаем.
Если у вас камера типа Pinhole, то правим TUM1.yaml:

nano  /home/pi/ORB_SLAM3/Examples/Monocular/TUM1.yaml

если fish-eye, в понятиях ORB_SLAM3 это тип KannalaBrandt8, то TUM-VI.yaml:

nano  /home/pi/ORB_SLAM3/Examples/Monocular/TUM-VI.yaml

В целом у приведенных TUM файлов разницы немного в части содержимого.

Необходимо снизить ORBextractor.nLevels, обратить внимание на fps, разрешение 512х512, а также ORBextractor.nFeatures — количество извлекаемых фич (здесь необходим баланс производительности/ качества — меньше фич — быстрее работает, но хуже позиционируется и наоборот).

TUM-VI.yaml

%YAML:1.0






------------------------------------------------------------------------------------------
## CATKIN INSTALL ROS PACKAGES
------------------------------------------------------------------------------------------

## METHOD #1 ==================================================

26) Чтобы сделать это правильно, создайте рабочую область для catkin для нашего пользовательского файла запуска:

$ mkdir -p ~/rosvid_ws/src 
$ cd ~/rosvid_ws 
$ catkin_make 
$ source devel/setup.bash

27) Затем создайте пакет ROS:
$ cd src
$ catkin_create_pkg vidsrv std_msgs rospy roscpp 


## METHOD #2 ==================================================


git clone https://github.com/IntelRealSense/realsense-ros.git
cd realsense-ros/
git checkout `git tag | sort -V | grep -P "^2.\d+\.\d+" | tail -1`
cd ../../


### THIS WORK ONLY WITHOUT CONDA ENV ###

catkin init
catkin clean
catkin config --cmake-args -DCATKIN_ENABLE_TESTING=False -DCMAKE_BUILD_TYPE=Release
### catkin config --install
catkin build

## METHOD #3 ==================================================
OR: ## https://github.com/ros-drivers/usb_cam/issues/351

I recommend to use 
>catkin-tools 

instead of 
>catkin_make 
also for the dependency installation, please use rosdep.



source devel/setup.bash

roslaunch realsense2_camera rs_camera.launch








------------------------------------------------------------------------------------------
## ros_astra_camera
------------------------------------------------------------------------------------------
# https://github.com/orbbec/ros_astra_camera

# Assuming you have sourced the ros environment, same below
sudo apt install libgflags-dev  ros-$ROS_DISTRO-image-geometry ros-$ROS_DISTRO-camera-info-manager\
ros-$ROS_DISTRO-image-transport ros-$ROS_DISTRO-image-publisher  libusb-1.0-0-dev libeigen3-dev
ros-$ROS_DISTRO-backward-ros libdw-dev





------------------------------------------------------------------------------------------


https://github.com/IntelRealSense/realsense-ros
### https://github.com/IntelRealSense/realsense-ros/issues/1432
https://github.com/IntelRealSense/realsense-ros/issues/1397


Answering my previous error not finding realsense2_camera package after building with catkin build command, I could solve it if I skip catkin config --install.
Then the whole processes are as follows. RealSense node was successfully build up.

mkdir -p (my_working_directory)/catkin_ws/src  

cd (my_working_directory)//catkin_ws/src/

git clone https://github.com/IntelRealSense/realsense-ros.git
cd realsense-ros/
git checkout `git tag | sort -V | grep -P "^2.\d+\.\d+" | tail -1`
cd ../../


### THIS WORK ONLY WITHOUT CONDA ENV ###

catkin init
catkin clean
catkin config --cmake-args -DCATKIN_ENABLE_TESTING=False -DCMAKE_BUILD_TYPE=Release
### catkin config --install
catkin build


OR: ## https://github.com/ros-drivers/usb_cam/issues/351

I recommend to use 
>catkin-tools 

instead of 
>catkin_make.
also for the dependency installation, please use rosdep.



source devel/setup.bash

roslaunch realsense2_camera rs_camera.launch

Anyway, if I insert catkin config --install just before catkin build, install folder was made. If I source install/setup.bash instead of devel/setup.bash in this case, roslaunch succeeded.
Which is better, and could you give me a clear explanation if possible?









++++++++



==========================================================================================
## ROS realsense2_camera
==========================================================================================
# https://github.com/IntelRealSense/realsense-ros/issues/1962



After setting 640x480 and 30fps for each I obtain the following summary:

>roslaunch realsense2_camera rs_camera.launch camera:=camera1 filters:=pointcloud depth_width:=640 color_width:=640 depth_height:=480 color_height:=480 depth_fps:=30 color_fps:=30 pointcloud_texture_stream:=RS2_STREAM_ANY


... logging to /home/ubuntu/.ros/log/e1423ed6-e1ca-11eb-969a-d38632247d1a/roslaunch-ubuntu-2239.log
Checking log directory for disk usage. This may take a while.
Press Ctrl-C to interrupt
Done checking log file disk usage. Usage is <1GB.

started roslaunch server http://192.168.0.113:45327/

>>>>work picture

Point Cloud Message appears in rviz however image_raw topic is still not received, however, the image_rect_raw topic is received.



==========================================================================================
## ROS make packages with catkin
==========================================================================================

jetson@ubuntu:~$ cd catkin_ws/src
jetson@ubuntu:~/catkin_ws/src$ git clone https://github.com/ros-drivers/usb_cam.git

Cloning into 'usb_cam'...
remote: Enumerating objects: 3228, done.
remote: Counting objects: 100% (687/687), done.
remote: Compressing objects: 100% (219/219), done.
remote: Total 3228 (delta 527), reused 468 (delta 468), pack-reused 2541 (from 2)
Receiving objects: 100% (3228/3228), 1.78 MiB | 1.40 MiB/s, done.
Resolving deltas: 100% (1536/1536), done.

### ERRRORR     

USE THIS:

cd ~/catkin_ws/src
git clone https://github.com/bosch-ros-pkg/usb_cam.git
cd ..

catkin_make

-- Generating done (0.0s)
-- Build files have been written to: /home/jetson/build

RUN:
source /home/jetson/catkin_ws/devel/setup.bash

bash -c "source /home/jetson/catkin_ws/devel/setup.bash; roslaunch usb_cam usb_cam-test.launch"






-- ~~ - realsense_camera_msgs (plain cmake)
-- ~~ - realsense2_camera (plain cmake)
-- ~~ - realsense2_node (plain cmake)

If you are trying to catkin_make clean these packages from ROS1 Kinetic, it probably won't work (these packages are meant for ROS2). Follow the steps in https://github.com/IntelRealSense/realsense-ros but omit the line: git checkout git tag | sort -V | grep -P "^\d+\.\d+\.\d+" | tail -1 and it should work



---------



the latest driver librealsense definitely outputs colored pointclouds. I tried to run on the Jetson nano

./pointcloud/rs-pointcloud


-----

hank you for your help. I think the problem was not solved yet. 
My workaround was to use a Jetson Nano and try to use the black and white pointclouds,
 but it will be great if the RGB would work. 
"building the earlier 2.41.0 version of librealsense and the matching 2.2.21 version 
of the RealSense ROS wrapper" didn't really work for me, but I can try more.




==========================================================================================

....


[100%] Linking CXX executable /home/jetson/_packages/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3/Mono
/usr/bin/ld: warning: libpango_display.so.0, needed by /home/jetson/_packages/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3/../../../lib/libORB_SLAM3.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libpango_vars.so.0, needed by /home/jetson/_packages/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3/../../../lib/libORB_SLAM3.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libpango_opengl.so.0, needed by /home/jetson/_packages/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3/../../../lib/libORB_SLAM3.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libpango_core.so.0, needed by /home/jetson/_packages/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3/../../../lib/libORB_SLAM3.so, not found (try using -rpath or -rpath-link)

[100%] Built target Mono




+++++++++++++++




## sudo sh -c 'echo "deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main" > /etc/apt/sources.list.d/ros2.list'

$jetson@ubuntu:~$ dpkg --print-architecture
arm64



# DNN Inference Nodes for ROS/ROS2
# https://github.com/dusty-nv/ros_deep_learning


# https://github.com/pythops/jetson-image
# Nvidia Jetson Minimalist Images

## REDIT about ROS2

I don't love RViz as a visualizer. I use Meshcat a lot. But you need to assemble further tools to compute the poses of the meshes on an articulated body from the joint values. robot_state_publisher, the tf transform tree, and RViz are there ready-to-go.

I personally use Pinocchio a lot for URDF parsing and computing poses for Meshcat, it's a nice alternative and easily installed. 





------------------------------------------------------------------------------------------
# INSTALL ORB_SLAM3 with ROS
------------------------------------------------------------------------------------------

# https://blog.csdn.net/qq_39537898/article/details/124775247







#--------------------------------------------------------------------------------------------
# Camera Parameters. Adjust them!
#--------------------------------------------------------------------------------------------
File.version: "1.0"

Camera.type: "KannalaBrandt8"

# Camera calibration and distortion parameters (OpenCV) 
Camera1.fx: 190.978477
Camera1.fy: 190.973307
Camera1.cx: 254.931706
Camera1.cy: 256.897442

# Equidistant distortion 0.0034823894022493434, 0.0007150348452162257, -0.0020532361418706202, 0.0002029$
Camera1.k1: 0.003482389402
Camera1.k2: 0.000715034845
Camera1.k3: -0.002053236141
Camera1.k4: 0.000202936736

# Camera resolution
Camera.width: 512
Camera.height: 512

# Camera frames per second 
Camera.fps: 20 
# Color order of the images (0: BGR, 1: RGB. It is ignored if images are grayscale)
Camera.RGB: 1
#--------------------------------------------------------------------------------------------
# ORB Parameters
#--------------------------------------------------------------------------------------------
# ORB Extractor: Number of features per image
ORBextractor.nFeatures: 1000 # Tested with 1250
# ORB Extractor: Scale factor between levels in the scale pyramid      
ORBextractor.nLevels: 3 #8

# ORB Extractor: Fast threshold
# Image is divided in a grid. At each cell FAST are extracted imposing a minimum response.
# Firstly we impose iniThFAST. If no corners are detected we impose a lower value minThFAST
# You can lower these values if your images have low contrast
ORBextractor.iniThFAST: 20
ORBextractor.minThFAST: 7

#--------------------------------------------------------------------------------------------
# Viewer Parameters
#--------------------------------------------------------------------------------------------
Viewer.KeyFrameSize: 0.05
Viewer.KeyFrameLineWidth: 1.0
Viewer.GraphLineWidth: 0.9
Viewer.PointSize: 2.0
Viewer.CameraSize: 0.08
Viewer.CameraLineWidth: 3.0
Viewer.ViewpointX: 0.0
Viewer.ViewpointY: -0.7
Viewer.ViewpointZ: -3.5
Viewer.ViewpointF: 500.0
Viewer.imageViewScale: 0.3



Запустим все (пока без ноды imu):

1-st terminal:

bash -c "source /opt/ros/noetic/setup.bash; roscore"

2-d:

bash -c "source /home/pi/catkin_ws/devel/setup.bash; roslaunch usb_cam usb_cam-test.launch"

либо


bash -c "source /home/pi/catkin_ws/devel/setup.bash; roslaunch raspicam_node camerav2_512x512_fish.launch"

3d:

bash -c "source /home/pi/catkin_ws/devel/setup.bash;cd /home/pi/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3;
./Mono /home/pi/ORB_SLAM3/Vocabulary/ORBvoc.txt /home/pi/ORB_SLAM3/Examples/Monocular/TUM-VI.yaml"

В целом, raspberry неплохо справляется:


Однако, при резких поворотах мощностей явно не хватает.

Тем не менее, если двигаться неспеша и не дергать камерой, то можно построить «маршрут»:








Так выглядит пройтись туда и обратно с плавным разворотом камеры.

Сохранить карту, нажав на кнопку gui не получится. Такой кнопки нет. Чтобы карта записалась необходимо перед стартом ORB_SLAM3 добавить строку в TUM.yaml файл:

System.SaveAtlasToFile: "my_01"
, где my_01 — название карты.
При этом карта будет сохраняться в /home/pi/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3 и называться my_01.osa.

Чтобы загрузить карту при старте ORB_SLAM, которая ранее была построена в том же TUM файле указать:

System.LoadAtlasFromFile: "my_01"


ORB_SLAM3 c imu.

Пример запуска с учетом imu будет выглядеть так:
bash -c -E "source /home/pi/catkin_ws/devel/setup.bash;cd /home/pi/ORB_SLAM3/Examples_old/ROS/ORB_SLAM3;
./Mono_Inertial /home/pi/ORB_SLAM3/Vocabulary/ORBvoc.txt /home/pi/ORB_SLAM3/Examples/Monocular-Inertial/TUM-VI_far.yaml"

*с учетом уже ранее запущенных нод в других терминалах roscore, raspicam_node и собственно imu:


Не лишним будет убедиться, что imu шлет сообщения в топик imu:
rostopic echo /imu




Содержимое
TUM-VI_far.yaml
%YAML:1.0
#------------------------------------------------------------------------------$
# Camera Parameters. Adjust them!
#------------------------------------------------------------------------------$
File.version: "1.0"
Camera.type: "KannalaBrandt8"

# Camera calibration and distortion parameters (OpenCV) 
Camera1.fx: 190.978477
Camera1.fy: 190.973307
Camera1.cx: 254.931706
Camera1.cy: 256.897442

# Equidistant distortion 0.0034823894022493434, 0.0007150348452162257, -0.00205$
#Camera.bFishEye: 1
Camera1.k1: 0.003482389402
Camera1.k2: 0.000715034845
Сamera1.k3: -0.002053236141
Camera1.k4: 0.000202936736

# Camera resolution
Camera.width: 512
Camera.height: 512

# Camera frames per second 
Camera.fps: 3

# Color order of the images (0: BGR, 1: RGB. It is ignored if images are grayscale)
Camera.RGB: 1

# Color order of the images (0: BGR, 1: RGB. It is ignored if images are grayscale)
Camera.RGB: 1

# Transformation from body-frame (imu) to camera
IMU.T_b_c1: !!opencv-matrix
   rows: 4
   cols: 4
   dt: f
   data: [-0.9995250378696743, 0.0075019185074052044, -0.02989013031643309, 0.045574835649698026,
          0.029615343885863205, -0.03439736061393144, -0.998969345370175, -0.071161801837997044,
         -0.008522328211654736, -0.9993800792498829, 0.03415885127385616, -0.044681254117144367,
          0.0, 0.0, 0.0, 1.0]
# IMU noise (Use those from VINS-mono)
IMU.NoiseGyro: 0.004 # 0.00016 (TUM) # 0.00016    # rad/s^0.5
IMU.NoiseAcc: 0.04  # 0.0028 (TUM) # 0.0028     # m/s^1.5
IMU.GyroWalk: 0.000022 #(VINS and TUM) rad/s^1.5
IMU.AccWalk: 0.0004 # 0.00086 # 0.00086    # m/s^2.5
IMU.Frequency: 100.0
System.thFarPoints: 3.0

#--------------------------------------------------------------------------------------------
# ORB Parameters
#--------------------------------------------------------------------------------------------

# ORB Extractor: Number of features per image
ORBextractor.nFeatures: 1000 # Tested with 1250
# ORB Extractor: Scale factor between levels in the scale pyramid       
ORBextractor.scaleFactor: 1.2

# ORB Extractor: Number of levels in the scale pyramid  
ORBextractor.nLevels: 3 #8

# ORB Extractor: Fast threshold
# Image is divided in a grid. At each cell FAST are extracted imposing a minimum response.
# Firstly we impose iniThFAST. If no corners are detected we impose a lower value minThFAST
# You can lower these values if your images have low contrast
ORBextractor.iniThFAST: 20
ORBextractor.minThFAST: 7
#--------------------------------------------------------------------------------------------
# Viewer Parameters
#--------------------------------------------------------------------------------------------
Viewer.KeyFrameSize: 0.05
Viewer.KeyFrameLineWidth: 1.0
Viewer.GraphLineWidth: 0.9
Viewer.PointSize: 2.0
Viewer.CameraSize: 0.08
Viewer.CameraLineWidth: 3.0
Viewer.ViewpointX: 0.0
Viewer.ViewpointY: -0.7
Viewer.ViewpointZ: -3.5
Viewer.ViewpointF: 500.0


Здесь необходимо обратить внимание на IMU.Frequency: 100.0 и System.thFarPoints: 3.0.
Частоту imu необходимо будет поменять в launch файле imu ноды с 200 на 100.
System.thFarPoints — это расстояние, дальше которого ORB_SLAM не будет извлекать фичи из изображения.

На этом пока все.
Теги:
orb-slam
raspberry pi
ros
ros noetic



------------------------------------------------------------------------------------------
# ORB_SLAM2_CUDA 
------------------------------------------------------------------------------------------
## https://github.com/thien94/ORB_SLAM2_CUDA

Modified version of ORB-SLAM2 with GPU enhancement and several ROS topics for NVIDIA Jetson TX1, TX2, Xavier, Nano. Currently only supports Monocular camera. Run in real time.

mplementation
 Monocular
 #Stereo
 #RGB-D

Published topics

tf
pose
pointcloud
current frame

Tested on:
Jetson TX1, TX2
Jetson Xavier
Jetson Nano

...

First, check to get the CUDA compiler version:

nvcc --version 

...

#Run non-ROS examples in Monocular node
Please refer to ORB-SLAM2 repo for a detailed step-by-step instruction, with two modifications:

The executable is located in the build folder instead of Examples/Monocular.
For TUM and KITTI examples, add a fourth argument at the end of the command, which corresponds to bUseViewer that enables / disables Viewer to pop up.
Example run:

$ cd /path/to/ORB_SLAM2_CUDA
$ ./build/mono_tum Vocabulary/ORBvoc.txt Examples/Monocular/TUM1.yaml Data/rgbd_dataset_freiburg1_desk true

___

Run ROS launch file for Monocular node
This one is created by me. Requires PCL library to run.

First you need to have the camera's image published on topic camera/image_raw.

Change the vocabulary and camera settings file accordingly. The directory is set in the launch file, located at ORB_SLAM2_CUDA/Examples/ROS/ORB_SLAM2_CUDA/launch/ros_mono.launch

Then launch:

roslaunch /path/to/ORB_SLAM2_CUDA/Examples/ROS/ORB_SLAM2_CUDA/launch/ros_mono.launch


This will run the ROS publisher node. The ROS topics will now be published in the ROS network. Run RVIZ for visualization:

rosrun rviz rviz

Note that Viewer is disable by default.

+++++++++


Yahboom is an excellent supplier. The Jetson Orin NX Dev Kit 16 Gb board arrived on time in 22 days, as they promised. Everything on the board corresponds to the description, 16 Gb memory, 256G SSD. I checked it, installed my software, everything works as it should.

The SSD disk already had the system pre-installed and a lot of the necessary software installed, this saved my time. Excellent support in the WhatsApp group, all questions are answered in detail and quickly. Since the product is very complex, this is necessary and Yahboom copes with it. Five stars Yahboom, I highly recommend it!
